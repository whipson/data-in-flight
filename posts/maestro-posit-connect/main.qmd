---
title: "Many Data Pipelines on Posit Connect with Maestro"
subtitle: How to deploy and monitor many data pipelines with maestro and Posit Connect
author: "Will Hipson"
date: "2024-10-02"
date-modified: last-modified
categories: [R, data engineering, data pipelines, orchestration, maestro, deployment, posit connect, tutorial]
draft: true
---

[Maestro](https://whipson.github.io/maestro/) is a package for scheduling, deploying, and monitoring data pipelines in a single project. Deploying maestro involves taking the project and automating it on some kind of schedule.

[Posit Connect](https://posit.co/products/enterprise/connect/) is a platform for deploying and sharing data science content developed by Posit. Connect is primarily used to host content for people to interact with (i.e., reports, dashboards, etc). I think it's great for this purpose and we use it this way at the airport all the time. However, less well known is it can be used to run data pipelines and even act as a data repository.[^1] This is because you can schedule the execution of Quarto documents in Connect; and this means you can schedule the execution of any arbitrary R code to run. This blew my mind when I first realized it!

[^1]: I hesitate to say 'database' because while it can store flat files (.csv and .parquet) it cannot be used to query them like in a true database.

Here we'll look at a real world example of using maestro and Posit Connect together to deploy multiple data pipelines that automatically extract data from a source and store it in tables that accumulate data over time.

## Maestro Project

We'll need to install `maestro` from CRAN first.

```{r}
#| eval: false
# install.packages("maestro")
library(maestro)
```

```{r}
#| eval: false
#' data_engineering_books_etl maestro pipeline
#'
#' @maestroFrequency 1 day
#' @maestroStartTime 2024-10-01
#' @maestroTz UTC
#' @maestroLogLevel INFO

data_engineering_books_etl <- function(board) {

  req <- httr2::request("https://openlibrary.org/") |>
    httr2::req_url_path_append("search.json") |>
    httr2::req_url_query(
      q = "subject:data+engineering",
      sort = "new",
      lang = "eng",
      limit = 1000
    )

  resp <- req |>
    httr2::req_perform() |>
    httr2::resp_body_json(simplifyVector = TRUE)

  docs <- resp$docs |>
    janitor::clean_names() |>
    dplyr::filter(language != "NULL") |>
    dplyr::rowwise() |>
    dplyr::mutate(
      authors = paste(author_name, collapse = ", ")
    ) |>
    dplyr::ungroup() |>
    dplyr::mutate(
      insert_time = lubridate::now()
    ) |>
    dplyr::select(
      title, authors, first_publish_year
    )

  # Attempt to read existing book titles
  tryCatch({
    existing_titles <- pins::pin_read(
      board,
      "data_engineering_book_titles_transactional"
    ) |>
      dplyr::pull(title)

    # Remove titles that already exist
    docs <- docs |>
      dplyr::filter(!title %in% existing_titles)
  })

  pin_append(
    board,
    docs,
    "data_engineering_book_titles_transactional"
  )
}
```

```{r}
#' Reads a pin (creates if not existing) appends new rows and 
#' writes it back to the same pin 
pin_append <- function(board, x, name, type = "parquet") {

  existing <- tryCatch({
    pins::pin_read(
      board,
      name
    )
  }, error = \(e) {
    warning("Pin '", name, "' does not exist. Creating.")
    return(NULL)
  })

  new <- dplyr::bind_rows(x, existing)

  tryCatch({
    pins::pin_write(
      board,
      new,
      name = name,
      type = type
    )
  }, error = \(e) {
    stop("Failed to append to pin '", name, "'")
  })
}
```

We'll use this function repeatedly throughout the project, so I'll put it in a folder called R.

### 
