[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Will, I’m a Data Engineer at the Halifax International Airport. I write about things that interest me in the world of data engineering and data science. I’m an enthusiastic R developer, and author of the maestro package.\nMost of my working hours are spent managing a data platform - building data pipelines, managing databases, and serving consumers of data. Outside of that I like reading, drawing, and watching spy films.\nWrites about:\n\nR\nData Engineering\nData Visualization"
  },
  {
    "objectID": "posts/prepare-for-takeoff/index.html",
    "href": "posts/prepare-for-takeoff/index.html",
    "title": "Prepare for Takeoff",
    "section": "",
    "text": "My new blog Data In Flight is about sharing my experiences working with real data on a small scale. In my experience I’ve found that exciting things happen when you have small-medium sized data and an even smaller budget. Most people don’t need the latest and greatest VC backed data platform, despite what tech influencers on LinkedIn say.\nThis blog is about scrappy data engineering with a team of one (or at most a number countable on a single hand). My natural inclination is to use R for everything so most of the code-related content in this blog will be in R.\nNothing here is intended to be polished or the final word (a near impossibility in a rapidly changing field). It’s about the journey and not the destination."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "The Data Decontamination Mindset\n\n\nWhy you should assume your organization’s data is radioactive\n\n\n\ndata cleaning\n\n\ndata engineering\n\n\ningestion\n\n\nopinion\n\n\n\n\n\n\n\n\n\nJul 18, 2024\n\n\nWill Hipson\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying a Maestro Project to Google Cloud Platform\n\n\nHow to deploy a maestro project to GCP\n\n\n\nR\n\n\ndata engineering\n\n\ndata pipelines\n\n\norchestration\n\n\nmaestro\n\n\ndeployment\n\n\ncloud\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\nWill Hipson\n\n\n\n\n\n\n\n\n\n\n\n\nHello maestro\n\n\nA new package for orchestrating data pipelines in R\n\n\n\nR\n\n\ndata engineering\n\n\ndata pipelines\n\n\norchestration\n\n\nmaestro\n\n\npackages\n\n\ndeployment\n\n\nrelease\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\nWill Hipson\n\n\n\n\n\n\n\n\n\n\n\n\nPrepare for Takeoff\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nWill Hipson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/data-contamination/main.html",
    "href": "posts/data-contamination/main.html",
    "title": "The Data Decontamination Mindset",
    "section": "",
    "text": "Data engineers deal with data from a variety of sources. Typically, you have very little control over these sources. Best case scenario is the data comes from an API that guarantees schema consistency and data quality to some degree. I love it when data is provided this way - it makes my little data engineer’s heart sing.\nIncreasingly, however, I’ve come across scenarios in our organization where people (non data-savvy folks) want their data to ingested in our data platform so that it can be integrated with downstream pipelines and applications. I’m all for this because it means our data platform is being used by people other than myself (and machine users). But it introduces data quality issues that you probably didn’t think about before - at least not when you’ve only been pulling data from a pristine third party API.\nHere I want to explain my new way of thinking about data that comes from within your own organization. This way of thinking states that you should treat all data that comes from people in your organization as contaminated. Radioactive. Not to be trusted. I call this the data decontamination mindset."
  },
  {
    "objectID": "posts/data-contamination/main.html#the-situation-let-the-people-write",
    "href": "posts/data-contamination/main.html#the-situation-let-the-people-write",
    "title": "The Data Decontamination Mindset",
    "section": "The Situation: Let the people write!",
    "text": "The Situation: Let the people write!\nAt the airport, we’re still in the early stages of rolling out a self-serve data platform for the organization. It’s mostly used internally so that people can do BI stuff. When we launched the platform, it didn’t really take off as expected. People were reluctant to self-serve1. Instead, they wanted to be able to write their own data to the data warehouse. There were two main reasons for this:\n\nSharing data with the organization in a more centralized location.\nSetting up a pipeline where they deposit their data somewhere (e.g., Dropbox, Sharepoint, etc.) and it’s automatically ingested and transformed to be more useful for them.\n\nWe certainly weren’t going to allow users free reign to create tables at their whim. But it made sense to have some way for people to contribute so long as it was relevant to the business.\nThe problem was their data was messy.\nIt was messy in ways that we never expected it to be. Specifically, there were duplicate records everywhere; but we didn’t check for this and it got entered into the platform anyway and it was only some time later that we uncovered it. Had we checked for problems at the outset we could’ve saved time and headaches.\nAnd for this reason, I changed the way I think about data coming from inside the business. I had to view it as contaminated by default."
  },
  {
    "objectID": "posts/data-contamination/main.html#symptoms-of-contaminated-data",
    "href": "posts/data-contamination/main.html#symptoms-of-contaminated-data",
    "title": "The Data Decontamination Mindset",
    "section": "Symptoms of Contaminated Data",
    "text": "Symptoms of Contaminated Data\nContaminated data is another term for messy data, but I think the term ‘contaminated’ helps us to think more critically about what data we allow into our data warehouses. What I call the symptoms of contaminated data are nothing new to anyone who’s thought about data quality2. Some symptoms are more problematic than others. Among the most egregious are the following:\n\nDuplicated records\nAll NA/NULL rows or columns (an indication of non-rectangular data)\nInconsistent date/datetime formatting (e.g., 2024-07-10, 08-10-24, …)\n\nThese are immediate cause for concern because they put into question the accuracy of the data.\nOther less obvious markers of contamination of the following:\n\nConstant columns (e.g., all values are 0)\nCharacter columns that are almost entirely numeric (could indicate a typo like 1O instead of 10)\nNon human-readable column names3\n\nAll these symptoms are typically caused by manual data entry in Excel spreadsheets. We’ve found that people outside of the data team understand their side of the business extremely well, but they unfortunately tend to conflate raw data with presentation. For instance, we regularly see Excel sheets using colours to convey information, the use of summary rows, and metadata/statistics calculated in cells located outside the rectangular structure of the data itself. These kinds of things are fine when you’re presenting data, but they’re a no go for automation."
  },
  {
    "objectID": "posts/data-contamination/main.html#quarantine-and-decontaminate",
    "href": "posts/data-contamination/main.html#quarantine-and-decontaminate",
    "title": "The Data Decontamination Mindset",
    "section": "Quarantine and Decontaminate",
    "text": "Quarantine and Decontaminate\nThe data decontamination process looks like this:\n\n\n\nData decontamination process\n\n\nWe first check the data for signs of contamination symptoms. If it fails, it goes into quarantine. Quarantined data is in a holding pattern until the owner helps it get better (i.e., by fixing the problems) or a case can be made for allowing exceptions (and it has to be a very strong case).\nIn our approach, quarantining is a first step to notify us and the owner of problems. Importantly, quarantine involves human intervention - the data team needs to give an ok for it to pass. If it passes the quarantine check, it can move on to the decontamination chamber. This is where we prepare data to be written into the platform such that it follows our conventions. Specifically, it undergoes the following:\n\nConversion of column names to snake_case\nDates and timestamp formats are converted to %Y-%m-%d and %Y-%m-%d %H:%M:%S\nLeading and trailing whitespace is removed\n\nThese are relatively minor changes that have very little impact on the data itself.4 The final check is then made when the data is actually inserted into the data warehouse. If the schema is invalid it will prevent data from entering. This is something we get for free because of the nature of the platform, but if we were just writing data to object storage we’d want steps in place to manually check the schema.\nSo now, after learning from our mistakes we’re starting to ingrain these practices in our data ingestion flows. I’ve developed an internal package called crosscheck to perform the checking steps and decontamination. I’m also looking at integrating our existing pointblank processes into this internal package.5"
  },
  {
    "objectID": "posts/data-contamination/main.html#decontamination-mindset",
    "href": "posts/data-contamination/main.html#decontamination-mindset",
    "title": "The Data Decontamination Mindset",
    "section": "Decontamination Mindset",
    "text": "Decontamination Mindset\nI’m suggesting to adopt a data decontamination mindset: immediately assume that data is contaminated. I think it is especially important to treat data coming from individuals as contaminated, particularly if the business doesn’t have strong data literacy. We usually can have a bit more trust in data coming from a reputable third-party API, but we should also apply a similar critical attitude in this case too.\nContaminated, poor quality data can have massive impacts downstream, so taking the time to decontaminate your data is critical. Don’t make the mistake I did and just trust that it’s all good because it’s coming from a domain expert."
  },
  {
    "objectID": "posts/data-contamination/main.html#footnotes",
    "href": "posts/data-contamination/main.html#footnotes",
    "title": "The Data Decontamination Mindset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m still grappling with why this is the case, but I think it’s primarily a literacy issue coupled with lack of time and motivation. This blog post by Briefer about the myth of self-serve rings true.↩︎\nI make the distinction between data quality which is domain agnostic and data validation which is tied to the business use case. Data quality is easier to determine because you look for the same kind of problems such as duplicated data, schema mismatch, NULL/NAs, etc. However, data validation asks more complex questions such as “do these values make sense given the business context?”.↩︎\nThis one is trickier and I’m more lax on it. In general, I think column names should be more descriptive and avoid the use of acronyms and abbreviations. Common acronyms in the business are obviously ok (in our case, the acronym IATA is widely understood). That said, if most of the columns are obscure it’s cause to reconsider the naming.↩︎\nData owners are still notified of changes. It may be the case that whitespace is used to differentiate values, but this is a red flag for us. Additionally, if changing column names to snake_case introduces column name duplications or invalid names, we stop and return to sender.↩︎\nI use the pointblank package to periodically check the quality of data already in our data warehouse. However, the steps laid out here would primarily be used for data that is preflight.↩︎"
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html",
    "href": "posts/hello-maestro/hello-maestro.html",
    "title": "Hello maestro",
    "section": "",
    "text": "I’m thrilled to announce the develop release of maestro a new package for developing, scheduling, and monitoring data pipelines. Here, I’ll walk through what maestro does and why you’d want to use it.\nremotes::install_github(\"https://github.com/whipson/maestro\")"
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html#the-motivation",
    "href": "posts/hello-maestro/hello-maestro.html#the-motivation",
    "title": "Hello maestro",
    "section": "The Motivation",
    "text": "The Motivation\nLet’s start with the why. Data engineers often have to manage multiple data pipelines - dozens or maybe even hundreds of pipelines. Scheduling and monitoring these pipelines in production poses challenges, particularly as the number of pipelines increases. Maestro takes a holistic approach by managing multiple independent pipelines in a single R project.1 No need to maintain several R projects or use bulky orchestration tools."
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html#one-orchestrator-many-pipelines",
    "href": "posts/hello-maestro/hello-maestro.html#one-orchestrator-many-pipelines",
    "title": "Hello maestro",
    "section": "One Orchestrator, Many Pipelines",
    "text": "One Orchestrator, Many Pipelines\nThere are two components to a maestro project: an orchestrator and a collection of pipelines. A pipeline is a function that performs a task, such as extracting data and then storing it somewhere. The goal of maestro is to manage (i.e., schedule and monitor) multiple pipelines. This is done using an orchestrator script. The orchestrator script runs maestro commands like build_schedule() and run_schedule() to execute the pipelines and monitor them."
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html#getting-started",
    "href": "posts/hello-maestro/hello-maestro.html#getting-started",
    "title": "Hello maestro",
    "section": "Getting Started",
    "text": "Getting Started\n\nlibrary(maestro)\n\nIt is best to use maestro in an R project. After installing the package, you can create a new project using create_maestro() or in RStudio &gt; New Project &gt; Maestro Project. You’ll find yourself in an R project with a structure like this:\nmy_project_directory\n├── orchestrator.R\n└── pipelines\n    └── my_pipe.R\n\nOur First Pipeline\nLet’s take a look at my_pipe.R that was created:\n\n#' my_pipe maestro pipeline\n#'\n#' @maestroFrequency 1 day\n#' @maestroStartTime 2024-06-03\n#' @maestroTz UTC\n#' @maestroLogLevel INFO\n\nmy_pipe &lt;- function() {\n\n  # Pipeline code\n}\n\nThe script has been populated with R code to assign a function called my_pipe. The function has no code in the body so it won’t do anything just yet. Above the function declaration are some specially formatted code comments. These are maestro tags and they’re used to configure the scheduling and operation of the pipeline.2\nThe tags most important for scheduling are maestroFrequency and maestroStartTime. maestroFrequency is how often to run the pipeline. It takes a number and a time unit, like 1 day, 3 hours, 15 minutes, or 6 months. maestroStartTime is a Date (yyyy-mm-dd) or timestamp (yyyy-mm-dd HH:MM:SS) indicating when the schedule starts.\nConfiguring the start time is important if you have specific times you want it to run. If, for example, you want the pipeline to run once daily at 12:30, you’d use @maestroStartTime 2024-06-03 12:00:00 (note here that the date part doesn’t matter unless you schedule it in the future).\nWe won’t concern ourselves with the other tags for now; just know that there are more and they all have default values.\nNow let’s get the pipeline to do something. In the spirit of typical data engineering tasks, we’ll create an ETL (Extract, Transform, Load) pipeline that gets data from a source, transforms it by adding a new column, and loads it into storage.\n\n\nMaking a Useful Pipeline\nWe’ll use the open API from Environment Canada called Geomet for meteorological data and we’ll use DuckDB for storage. We’ll need the httr2 and duckdb packages for extraction and storage, respectively.\n\n#' Located in ./pipelines/my_pipe.R\n#' @maestroFrequency 1 day\n#' @maestroStartTime 2024-06-03\ngeomet_stations &lt;- function() {\n  \n  # Formulate the request  \n  req &lt;- httr2::request(\n    \"https://api.weather.gc.ca/collections/climate-stations/items\"\n  ) |&gt; \n    httr2::req_url_query(\n      limit = 1000,\n      skipGeometry = TRUE\n    )\n  \n  # Send the request and interpret the response\n  resp &lt;- req |&gt; \n    httr2::req_perform() |&gt; \n    httr2::resp_body_json(simplifyVector = TRUE)\n  \n  # Get the properties element where the rectangular data is located\n  stations_dat &lt;- resp$features$properties\n  \n  # Clean the names\n  stations_clean &lt;- stations_dat |&gt; \n    janitor::clean_names() |&gt; \n    janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n    dplyr::mutate(\n      insert_time = lubridate::now(tzone = \"UTC\")\n    )\n  \n  # Connect to a local in-memory duckdb\n  conn &lt;- DBI::dbConnect(duckdb::duckdb())\n  \n  # Create and write to a table\n  DBI::dbWriteTable(\n    conn, \n    name = \"geomet_stations_transactional\", \n    value = stations_clean\n  )\n  \n  # Test that it worked in the return\n  res &lt;- DBI::dbGetQuery(\n    conn, \n    statement = \"\n    select * from geomet_stations_transactional\n    order by insert_time desc\n    limit 10\n    \"\n  ) |&gt;\n    dplyr::as_tibble()\n  \n  DBI::dbDisconnect(conn)\n  \n  return(\n    res\n  )\n}\n\n\n\nOrchestrate It\nNow that we have a single useful pipeline, let’s orchestrate it (in practice, we’d probably have more than one pipeline). We’ll set the orchestrator to run at a daily frequency (this does not actually cause it to run daily, we need something else external to the R project to actually run it). For testing purposes, we’ll then run this interactively:\n\n# Located in ./orchestrator.R\nlibrary(maestro)\n\nschedule &lt;- build_schedule()\n\norch_result &lt;- run_schedule(\n  schedule,\n  orch_frequency = \"1 day\"\n)\n\nℹ 1 script successfully parsed                                                  \n                                                                                \n── Running pipelines ▶                                                          \n✔ ./pipelines/my_pipe.R geomet_stations [749ms]                                 \n                                                                                \n── Pipeline execution completed ■ | 0.761 sec elapsed                           \n✔ 1 success | → 0 skipped | ! 0 warnings | ✖ 0 errors | ◼ 1 total               \n────────────────────────────────────────────────────────────────────────────────\n                                                                                \n── Next scheduled pipelines ❯                                                   \nPipe name | Next scheduled run                                                  \n• geomet_stations | 2024-06-21                                                  \n\n\n\n\n\nWhat we get back\nWe can see from the console output that the pipeline ran successfully. If we save it to a variable we get back a list with elements called and status and artifacts.\n\n$status\nThe $status element is a data.frame where each row is a pipeline. It has information about the status and runtime of each pipeline.\n\norch_result$status\n\n# A tibble: 1 × 10\n  pipe_name  script_path invoked success pipeline_started    pipeline_ended     \n  &lt;chr&gt;      &lt;chr&gt;       &lt;lgl&gt;   &lt;lgl&gt;   &lt;dttm&gt;              &lt;dttm&gt;             \n1 geomet_st… ./pipeline… TRUE    TRUE    2024-06-19 14:50:51 2024-06-19 14:50:51\n# ℹ 4 more variables: errors &lt;int&gt;, warnings &lt;int&gt;, messages &lt;int&gt;,\n#   next_run &lt;dttm&gt;\n\n\n\n\n$artifacts\n$artifacts is where any return values from the pipelines will be. In our case, it’s the test sample of data inserted into the table:\n\norch_result$artifacts[[1]]\n\n# A tibble: 10 × 33\n   wmo_identifier   longitude fre_stn_operator_name                 display_code\n   &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;                                        &lt;int&gt;\n 1 &lt;NA&gt;           -1114200000 &lt;NA&gt;                                             8\n 2 &lt;NA&gt;           -1140700000 &lt;NA&gt;                                             4\n 3 &lt;NA&gt;           -1114500000 &lt;NA&gt;                                             9\n 4 &lt;NA&gt;           -1120200000 &lt;NA&gt;                                             4\n 5 &lt;NA&gt;           -1105000000 &lt;NA&gt;                                             6\n 6 71634           -653552000 Environnement et Changement climatiq…           NA\n 7 &lt;NA&gt;            -633931040 &lt;NA&gt;                                            NA\n 8 &lt;NA&gt;            -640340090 Réseau coopératif de stations climat…           NA\n 9 71603           -660517000 NAV Canada                                      NA\n10 71706           -630709000 NAV Canada                                      NA\n# ℹ 29 more variables: eng_stn_operator_acronym &lt;chr&gt;,\n#   fre_stn_operator_acronym &lt;chr&gt;, mly_first_date &lt;chr&gt;, hly_first_date &lt;chr&gt;,\n#   fre_prov_name &lt;chr&gt;, country &lt;chr&gt;, climate_identifier &lt;chr&gt;,\n#   publication_code &lt;int&gt;, timezone &lt;chr&gt;, last_date &lt;chr&gt;,\n#   hly_last_date &lt;chr&gt;, has_hourly_data &lt;chr&gt;, elevation &lt;chr&gt;, stn_id &lt;int&gt;,\n#   dly_first_date &lt;chr&gt;, dly_last_date &lt;chr&gt;, eng_stn_operator_name &lt;chr&gt;,\n#   station_name &lt;chr&gt;, eng_prov_name &lt;chr&gt;, mly_last_date &lt;chr&gt;, …"
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html#deployment",
    "href": "posts/hello-maestro/hello-maestro.html#deployment",
    "title": "Hello maestro",
    "section": "Deployment",
    "text": "Deployment\nOk, so we ran the orchestrator interactively, but this is only useful for testing. In practice, we want to deploy this on a server and have it run every day. To be clear: maestro does not do this for you - it just assumes that you are doing this and behaves accordingly. In other words, when you declare orch_frequency = \"1 day\" you are saying that you intend to run the orchestrator every 1 day.\nThe first decision to make about deployment is local vs. cloud. If you own the server it’s local3, if you rent the hardware and connect to it remotely it’s cloud. Here, we’ll run through a straightforward local deployment because it requires less configuration and won’t cost you anything.\n\nMac/Linux: cronR\nCron is a job scheduler for the Mac/Linux systems. You can use cronR to interface with it via R.\n\nlibrary(cronR)\n\ncmd &lt;- cron_rscript(\n  \"orchestrator.R\", \n  workdir = getwd()\n)\n\ncron_add(\n  cmd, \n  frequency = \"daily\",\n  id = \"maestro\",\n  ask = FALSE\n)\n\n\n\nWindows: taskscheduleR\nWindows users can use taskscheduleR to schedule a job via R:\n\nlibrary(taskscheduleR)\n\ntaskscheduler_create(\n  taskname = \"maestro\", \n  rscript = \"orchestrator.R\", \n  schedule = \"DAILY\",\n  exec_path = getwd()\n)"
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html#final-remarks",
    "href": "posts/hello-maestro/hello-maestro.html#final-remarks",
    "title": "Hello maestro",
    "section": "Final Remarks",
    "text": "Final Remarks\nIn this post, we saw how maestro can be used to orchestrate multiple pipelines in a single R project. Maestro is in its early development, but I encourage you to try it out. We’re using it in production to orchestrate 18 production pipelines at the Halifax International Airport!\nI’d be remiss not to mention a few caveats of maestro:\n\nIt should only be used for light-medium scale batch jobs. If you need to do streaming and/or heavy load jobs, it’s probably not for you.\nConfiguring the schedule for the orchestrator requires some thought. If you have several pipelines at different intervals and times you’ll want to choose a frequency that makes sense. You can use suggest_orch_frequency() to get a heuristic suggestion based on a schedule. Even then - you need to make sure you actually run the orchestrator at this frequency. Weird things happen if your orchestrator actually runs more or less frequently than you said it would.\n\nThat said, I think maestro is great for small-medium sized pipeline orchestration. If you’re looking to deploy maestro on the cloud, this blog post will help you get started in Google Compute Platform (GCP).\n\nNotes\nThis post was created using R version 4.3.3 (2024-02-29) and maestro version 0.1.1."
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html#footnotes",
    "href": "posts/hello-maestro/hello-maestro.html#footnotes",
    "title": "Hello maestro",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA popular R package for data pipelines that has been around for some time is targets. Maestro is different from targets in that the focus of maestro is to orchestrate multiple independent data pipelines whereas targets is for a single connected pipeline. Maestro would be more suited toward ETL data engineering where targets is for complex analytics pipelines.↩︎\nR users will recognize that maestro uses custom roxygen2 tags.↩︎\nYes, this includes your personal laptop or desktop; however, it probably goes to sleep when not used. If you’re using a Mac, you can use pmset in the command line to get around this (https://www.dssw.co.uk/reference/pmset/).↩︎"
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "",
    "text": "In the previous post we saw how maestro helps orchestrate data pipelines in a single R project. For maestro to be useful you need to deploy it somewhere and these days that ‘somewhere’ is often cloud-based. Here we’ll walk through deploying a maestro project in the cloud on Google Cloud Platform (GCP).1\nThis post assumes some familiarity with the maestro package although the practice of deploying on GCP can be generalized to almost any project. The previous blog post walks through maestro specifically.\nThis project is available on Github here."
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#design",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#design",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Design",
    "text": "Design\nLike any cloud offering, GCP gives us many different ways to solve the same problem.2 My suggested design is by no means the only way nor is it necessarily the best way. I strove for simplicity. In the end, I landed on 3 GCP services needed to deploy the project:\n\nCloud Build for hosting the containerized project.\nCloud Run Jobs for executing the container.\nCloud Scheduler for scheduling the container to run.\n\nThe most complicated of these is the first step and requires some familiarity with Docker (and ideally Github)."
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#dockerizing-maestro",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#dockerizing-maestro",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Dockerizing Maestro",
    "text": "Dockerizing Maestro\nThe best way to ensure that our maestro project runs in the cloud with all its dependencies is to use Docker. Docker is widespread in software development these days and is practically a requirement for deploying anything. I won’t go over the fundamentals of Docker here (Alex Gold’s DevOps for Data Science has a Demystifying Docker chapter that is very helpful if you’re new or rusty with Docker).\nWe need to create a Dockerfile at the root level of the project. I used dockerfiler for this. It has a function dock_from_renv() and it got me about 90% of the way there. As implied in the name you need to be using renv.3\nAfter using dockerfiler to generate the Dockerfile, I made some necessary tweaks. The main thing is to add a few system libraries and to add the ENTRYPOINT [\"Rscript\", \"orchestrator.R\"] so that Google Cloud Run knows to execute that script:\n# Dockerfile\nFROM rocker/r-ver\nRUN apt-get update && apt-get install -y \\\n    libxml2-dev \\\n    libcurl4-openssl-dev \\\n    libssl-dev \\\n    libsodium-dev \\\n    zlib1g-dev \\\n    pkg-config \\\n    && rm -rf /var/lib/apt/lists/*\nRUN R -e 'install.packages(\"remotes\")'\nRUN R -e 'remotes::install_version(\"renv\", version = \"1.0.7\")'\nWORKDIR /usr/src/app\nCOPY . .\nRUN R -e 'renv::restore()'\nENTRYPOINT [\"Rscript\", \"orchestrator.R\"]\nI also had to add some system libraries (libcurl4-openssl-dev for making the API requests). This part is a bit tedious and will depend on your situation. For instance, if you’re doing anything geospatial you may need lib-gdal. ChatGPT was pretty helpful when I ran into errors building my docker image.\nThis leads us to the next step: building the image and testing it in a container locally. This is where you can debug problems before you go to deploy to GCP.\n\ndocker build -t maestro_gcp .\ndocker run maestro_gcp --rm"
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#creating-a-cloudbuild.yaml",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#creating-a-cloudbuild.yaml",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Creating a cloudbuild.yaml",
    "text": "Creating a cloudbuild.yaml\nThe cloudbuild.yaml is a configuration file for telling GCP what image to use and how to build it. Think of it as the link connecting your project to the place where the docker image will live in GCP. Full confession: ChatGPT helped me with this one:\nsteps:\n- name: 'gcr.io/cloud-builders/docker'\n  args: ['build', '-t', 'gcr.io/$PROJECT_ID/maestro-gcp', '.']\nimages:\n- 'gcr.io/$PROJECT_ID/maestro-gcp'\noptions:\n  logging: CLOUD_LOGGING_ONLY\nName this file cloudbuild.yaml and add it to the root level of your project. At this point, you should have a project structure that looks something like this:\n.\n├── Dockerfile\n├── cloudbuild.yaml\n├── maestro-gcp-deploy.Rproj\n├── orchestrator.R\n├── pipelines\n├── renv\n└── renv.lock"
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#hosting-the-container-on-gcp",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#hosting-the-container-on-gcp",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Hosting the Container on GCP",
    "text": "Hosting the Container on GCP\nThe great thing about Cloud Build is you can host a project with a Dockerfile on Github and deploy it to GCP from there.\nCreate the Github repo and push the project code there. Then, in the GCP console and, inside of Cloud Build &gt; Triggers, select Connect Repository.\nOnce you open the Connect Repository wizard and select Github, it’ll step you through some authentication.\n\n\n\nSuccessful Cloud Build in GCP Console."
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#create-cloud-job-run",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#create-cloud-job-run",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Create Cloud Job Run",
    "text": "Create Cloud Job Run\nIf your build was successful, congrats! You made it through the hardest part. We now go to Cloud Run Jobs.4 Go to Create Job and browse through to find your recently build container.\n\n\n\nCreate Job wizard in GCP Console.\n\n\nYou may also want to select the box to ‘Execute job immediately’ to test that the container runs as expected."
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#schedule-it",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#schedule-it",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Schedule It",
    "text": "Schedule It\nWe can stay right within the Cloud Run service to do the scheduling even though it is handled by Google Cloud Scheduler.\nClick on your newly created job and then on Triggers &gt; Add Scheduler Trigger:\n\n\n\nAdding a scheduler trigger to the cloud job.\n\n\nThe Frequency input uses cron syntax. You can use Crontab to help specify the schedule. When it’s created you should see something below like this:\n\n\n\nSuccessfully created cron schedule for Cloud Run Job.\n\n\nI like to trigger the job manually too to see how it runs, or you can just wait until the schedule kicks in.\n\n\n\nLogs of the cloud run job."
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#final-remarks",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#final-remarks",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Final Remarks",
    "text": "Final Remarks\nAnd that about sums it up for the deployment. Cloud deployment is almost never a straightforward process, so be prepared for some failed attempts. I relied on ChatGPT when I ran into problems. My organization gives us access to ChatGPT 4o and it seems to have an excellent grasp of GCP. It tends to provide command line instructions rather than point-click in the GCP console, but this is for your benefit - it’s more reproducible and stable."
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#footnotes",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#footnotes",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy GCP and not AWS or Azure? We chose GCP for this initial post on deployment for two reasons. First is it has always free tiers for commonly used services. For lightweight use cases, you won’t have to worry about incurring a cloud bill. Another reason is I’ve found the learning curve for GCP much gentler compared to AWS and Azure. In general, GCP is much better at helping you do what you need to do at very little to no cost (for lightweight jobs).↩︎\nCloud providers are great at packaging the same fundamental activity in a million different ways. Try not to get overwhelmed with all the different services in GCP. Usually you’ll only ever make use of a few of the most common ones. Everything comes down to compute and storage.↩︎\nYes, I realize this may feel like a lot before we’ve even made it to GCP. While Docker and renv create additional work (and often frustration) up front, it truly is more beneficial when it comes to the actual deployment. Without Docker, setting up the runtime is an especially painful game of whack-a-mole. This doesn’t mean you won’t have to whack a few moles first though.↩︎\nMake sure you choose Create Job and not Create Service. Services listen for HTTP requests whereas jobs can be triggered by a cron schedule.↩︎"
  },
  {
    "objectID": "posts/data-contamination/main.html#data-decontamination-mindset",
    "href": "posts/data-contamination/main.html#data-decontamination-mindset",
    "title": "The Data Decontamination Mindset",
    "section": "Data Decontamination Mindset",
    "text": "Data Decontamination Mindset\nI’m suggesting to adopt a data decontamination mindset: immediately assume that data is contaminated. I think it is especially important to treat data coming from individuals as contaminated, particularly if the business doesn’t have strong data literacy. We usually can have a bit more trust in data coming from a reputable third-party API, but we should also apply a similar critical attitude in this case too.\nContaminated, poor quality data can have massive impacts downstream, so taking the time to decontaminate your data is critical. Don’t make the mistake I did and blindly trust that it’s all good because it’s coming from a domain expert. In the end, you should always assume the worst but hope for the best."
  }
]