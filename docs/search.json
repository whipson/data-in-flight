[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Will, I’m a Data Engineer at the Halifax International Airport. I write about things that interest me in the world of data engineering and data science. I’m an enthusiastic R developer, and author of the maestro package.\nMost of my working hours are spent managing a data platform - building data pipelines, managing databases, and serving consumers of data. Outside of that I like reading, drawing, and watching spy films.\nWrites about:\n\nR\nData Engineering\nData Visualization"
  },
  {
    "objectID": "posts/prepare-for-takeoff/index.html",
    "href": "posts/prepare-for-takeoff/index.html",
    "title": "Prepare for Takeoff",
    "section": "",
    "text": "My new blog Data In Flight is about sharing my experiences working with real data on a small scale. In my experience I’ve found that exciting things happen when you have small-medium sized data and an even smaller budget. Most people don’t need the latest and greatest VC backed data platform, despite what tech influencers on LinkedIn say.\nThis blog is about scrappy data engineering with a team of one (or at most a number countable on a single hand). My natural inclination is to use R for everything so most of the code-related content in this blog will be in R.\nNothing here is intended to be polished or the final word (a near impossibility in a rapidly changing field). It’s about the journey and not the destination."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "maestro 0.6.0\n\n\nNew capabilities for flagging pipelines, setting priorities, and optimizing your orchestration\n\n\n\nR\n\ndata pipelines\n\norchestration\n\nmaestro\n\npackages\n\nrelease\n\n\n\n\n\n\n\n\n\nMay 15, 2025\n\n\nWill Hipson\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncing… schematic\n\n\nA new solution for data validation with a focus on user-facing messages\n\n\n\ndata cleaning\n\nR\n\npackages\n\nrelease\n\nschematic\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nWill Hipson\n\n\n\n\n\n\n\n\n\n\n\n\nYou Don’t Need Airflow: Orchestrate Many Data Flows in R with Maestro\n\n\n(At a fraction of the time and cost)\n\n\n\nR\n\ndata pipelines\n\norchestration\n\nmaestro\n\npackages\n\n\n\n\n\n\n\n\n\nFeb 26, 2025\n\n\nWill Hipson\n\n\n\n\n\n\n\n\n\n\n\n\nChain your maestro pipelines DAG style\n\n\nVersion 0.4.0 introduces tags for creating connected pipelines\n\n\n\nR\n\ndata pipelines\n\norchestration\n\nmaestro\n\npackages\n\nrelease\n\n\n\n\n\n\n\n\n\nNov 22, 2024\n\n\nWill Hipson\n\n\n\n\n\n\n\n\n\n\n\n\nmaestro 0.3.0\n\n\nNew methods for getting schedule status and return values from pipelines\n\n\n\nR\n\ndata pipelines\n\norchestration\n\nmaestro\n\npackages\n\nrelease\n\n\n\n\n\n\n\n\n\nSep 23, 2024\n\n\nWill Hipson\n\n\n\n\n\n\n\n\n\n\n\n\nmaestro 0.2.0\n\n\nNew tags for specifying hours, days, and months for pipelines\n\n\n\nR\n\ndata pipelines\n\norchestration\n\nmaestro\n\npackages\n\nrelease\n\n\n\n\n\n\n\n\n\nAug 27, 2024\n\n\nWill Hipson\n\n\n\n\n\n\n\n\n\n\n\n\nThe Data Decontamination Mindset\n\n\nWhy you should assume your organization’s data is radioactive\n\n\n\ndata cleaning\n\ndata engineering\n\ningestion\n\nopinion\n\n\n\n\n\n\n\n\n\nJul 18, 2024\n\n\nWill Hipson\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying a Maestro Project to Google Cloud Platform\n\n\nHow to deploy a maestro project to GCP\n\n\n\nR\n\ndata engineering\n\ndata pipelines\n\norchestration\n\nmaestro\n\ndeployment\n\ncloud\n\ntutorial\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\nWill Hipson\n\n\n\n\n\n\n\n\n\n\n\n\nHello maestro\n\n\nA new package for orchestrating data pipelines in R\n\n\n\nR\n\ndata engineering\n\ndata pipelines\n\norchestration\n\nmaestro\n\npackages\n\ndeployment\n\nrelease\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\nWill Hipson\n\n\n\n\n\n\n\n\n\n\n\n\nPrepare for Takeoff\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nWill Hipson\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/data-contamination/main.html",
    "href": "posts/data-contamination/main.html",
    "title": "The Data Decontamination Mindset",
    "section": "",
    "text": "Data engineers deal with data from a variety of sources. Typically, you have very little control over these sources. Best case scenario is the data comes from an API that guarantees schema consistency and data quality to some degree. I love it when data is provided this way - it makes my little data engineer’s heart sing.\nIncreasingly, however, I’ve come across scenarios in our organization where people (non data-savvy folks) want their data to ingested in our data platform so that it can be integrated with downstream pipelines and applications. I’m all for this because it means our data platform is being used by people other than myself (and machine users). But it introduces data quality issues that you probably didn’t think about before - at least not when you’ve only been pulling data from a pristine third party API.\nHere I want to explain my new way of thinking about data that comes from within your own organization. This way of thinking states that you should treat all data that comes from people in your organization as contaminated. Radioactive. Not to be trusted. I call this the data decontamination mindset."
  },
  {
    "objectID": "posts/data-contamination/main.html#the-situation-let-the-people-write",
    "href": "posts/data-contamination/main.html#the-situation-let-the-people-write",
    "title": "The Data Decontamination Mindset",
    "section": "The Situation: Let the people write!",
    "text": "The Situation: Let the people write!\nAt the airport, we’re still in the early stages of rolling out a self-serve data platform for the organization. It’s mostly used internally so that people can do BI stuff. When we launched the platform, it didn’t really take off as expected. People were reluctant to self-serve1. Instead, they wanted to be able to write their own data to the data warehouse. There were two main reasons for this:\n\nSharing data with the organization in a more centralized location.\nSetting up a pipeline where they deposit their data somewhere (e.g., Dropbox, Sharepoint, etc.) and it’s automatically ingested and transformed to be more useful for them.\n\nWe certainly weren’t going to allow users free reign to create tables at their whim. But it made sense to have some way for people to contribute so long as it was relevant to the business.\nThe problem was their data was messy.\nIt was messy in ways that we never expected it to be. Specifically, there were duplicate records everywhere; but we didn’t check for this and it got entered into the platform anyway and it was only some time later that we uncovered it. Had we checked for problems at the outset we could’ve saved time and headaches.\nAnd for this reason, I changed the way I think about data coming from inside the business. I had to view it as contaminated by default."
  },
  {
    "objectID": "posts/data-contamination/main.html#symptoms-of-contaminated-data",
    "href": "posts/data-contamination/main.html#symptoms-of-contaminated-data",
    "title": "The Data Decontamination Mindset",
    "section": "Symptoms of Contaminated Data",
    "text": "Symptoms of Contaminated Data\nContaminated data is another term for messy data, but I think the term ‘contaminated’ helps us to think more critically about what data we allow into our data warehouses. What I call the symptoms of contaminated data are nothing new to anyone who’s thought about data quality2. Some symptoms are more problematic than others. Among the most egregious are the following:\n\nDuplicated records\nAll NA/NULL rows or columns (an indication of non-rectangular data)\nInconsistent date/datetime formatting (e.g., 2024-07-10, 08-10-24, …)\n\nThese are immediate cause for concern because they put into question the accuracy of the data.\nOther less obvious markers of contamination of the following:\n\nConstant columns (e.g., all values are 0)\nCharacter columns that are almost entirely numeric (could indicate a typo like 1O instead of 10)\nNon human-readable column names3\n\nAll these symptoms are typically caused by manual data entry in Excel spreadsheets. We’ve found that people outside of the data team understand their side of the business extremely well, but they unfortunately tend to conflate raw data with presentation. For instance, we regularly see Excel sheets using colours to convey information, the use of summary rows, and metadata/statistics calculated in cells located outside the rectangular structure of the data itself. These kinds of things are fine when you’re presenting data, but they’re a no go for automation."
  },
  {
    "objectID": "posts/data-contamination/main.html#quarantine-and-decontaminate",
    "href": "posts/data-contamination/main.html#quarantine-and-decontaminate",
    "title": "The Data Decontamination Mindset",
    "section": "Quarantine and Decontaminate",
    "text": "Quarantine and Decontaminate\nThe data decontamination process looks like this:\n\n\n\nData decontamination process\n\n\nWe first check the data for signs of contamination symptoms. If it fails, it goes into quarantine. Quarantined data is in a holding pattern until the owner helps it get better (i.e., by fixing the problems) or a case can be made for allowing exceptions (and it has to be a very strong case).\nIn our approach, quarantining is a first step to notify us and the owner of problems. Importantly, quarantine involves human intervention - the data team needs to give an ok for it to pass. If it passes the quarantine check, it can move on to the decontamination chamber. This is where we prepare data to be written into the platform such that it follows our conventions. Specifically, it undergoes the following:\n\nConversion of column names to snake_case\nDates and timestamp formats are converted to %Y-%m-%d and %Y-%m-%d %H:%M:%S\nLeading and trailing whitespace is removed\n\nThese are relatively minor changes that have very little impact on the data itself.4 The final check is then made when the data is actually inserted into the data warehouse. If the schema is invalid it will prevent data from entering. This is something we get for free because of the nature of the platform, but if we were just writing data to object storage we’d want steps in place to manually check the schema.\nSo now, after learning from our mistakes we’re starting to ingrain these practices in our data ingestion flows. I’ve developed an internal package called crosscheck to perform the checking steps and decontamination. I’m also looking at integrating our existing pointblank processes into this internal package.5"
  },
  {
    "objectID": "posts/data-contamination/main.html#decontamination-mindset",
    "href": "posts/data-contamination/main.html#decontamination-mindset",
    "title": "The Data Decontamination Mindset",
    "section": "Decontamination Mindset",
    "text": "Decontamination Mindset\nI’m suggesting to adopt a data decontamination mindset: immediately assume that data is contaminated. I think it is especially important to treat data coming from individuals as contaminated, particularly if the business doesn’t have strong data literacy. We usually can have a bit more trust in data coming from a reputable third-party API, but we should also apply a similar critical attitude in this case too.\nContaminated, poor quality data can have massive impacts downstream, so taking the time to decontaminate your data is critical. Don’t make the mistake I did and just trust that it’s all good because it’s coming from a domain expert."
  },
  {
    "objectID": "posts/data-contamination/main.html#footnotes",
    "href": "posts/data-contamination/main.html#footnotes",
    "title": "The Data Decontamination Mindset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m still grappling with why this is the case, but I think it’s primarily a literacy issue coupled with lack of time and motivation. This blog post by Briefer about the myth of self-serve rings true.↩︎\nI make the distinction between data quality which is domain agnostic and data validation which is tied to the business use case. Data quality is easier to determine because you look for the same kind of problems such as duplicated data, schema mismatch, NULL/NAs, etc. However, data validation asks more complex questions such as “do these values make sense given the business context?”.↩︎\nThis one is trickier and I’m more lax on it. In general, I think column names should be more descriptive and avoid the use of acronyms and abbreviations. Common acronyms in the business are obviously ok (in our case, the acronym IATA is widely understood). That said, if most of the columns are obscure it’s cause to reconsider the naming.↩︎\nData owners are still notified of changes. It may be the case that whitespace is used to differentiate values, but this is a red flag for us. Additionally, if changing column names to snake_case introduces column name duplications or invalid names, we stop and return to sender.↩︎\nI use the pointblank package to periodically check the quality of data already in our data warehouse. However, the steps laid out here would primarily be used for data that is preflight.↩︎"
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html",
    "href": "posts/hello-maestro/hello-maestro.html",
    "title": "Hello maestro",
    "section": "",
    "text": "I’m thrilled to announce the release of maestro, a new package for developing, scheduling, and monitoring data pipelines. Here, I’ll walk through what maestro does and why you’d want to use it.\ninstall.packages(\"maestro\")"
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html#the-motivation",
    "href": "posts/hello-maestro/hello-maestro.html#the-motivation",
    "title": "Hello maestro",
    "section": "The Motivation",
    "text": "The Motivation\nLet’s start with the why. Data engineers often have to manage multiple data pipelines - dozens or maybe even hundreds of pipelines. Scheduling and monitoring these pipelines in production poses challenges, particularly as the number of pipelines increases. Maestro takes a holistic approach by managing multiple independent pipelines in a single R project.1 No need to maintain several R projects or use bulky orchestration tools."
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html#one-orchestrator-many-pipelines",
    "href": "posts/hello-maestro/hello-maestro.html#one-orchestrator-many-pipelines",
    "title": "Hello maestro",
    "section": "One Orchestrator, Many Pipelines",
    "text": "One Orchestrator, Many Pipelines\nThere are two components to a maestro project: an orchestrator and a collection of pipelines. A pipeline is a function that performs a task, such as extracting data and then storing it somewhere. The goal of maestro is to manage (i.e., schedule and monitor) multiple pipelines. This is done using an orchestrator script. The orchestrator script runs maestro commands like build_schedule() and run_schedule() to execute the pipelines and monitor them."
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html#getting-started",
    "href": "posts/hello-maestro/hello-maestro.html#getting-started",
    "title": "Hello maestro",
    "section": "Getting Started",
    "text": "Getting Started\n\nlibrary(maestro)\n\nIt is best to use maestro in an R project. After installing the package, you can create a new project using create_maestro() or in RStudio &gt; New Project &gt; Maestro Project. You’ll find yourself in an R project with a structure like this:\nmy_project_directory\n├── orchestrator.R\n└── pipelines\n    └── my_pipe.R\n\nOur First Pipeline\nLet’s take a look at my_pipe.R that was created:\n\n#' my_pipe maestro pipeline\n#'\n#' @maestroFrequency 1 day\n#' @maestroStartTime 2024-06-03\n#' @maestroTz UTC\n#' @maestroLogLevel INFO\n\nmy_pipe &lt;- function() {\n\n  # Pipeline code\n}\n\nThe script has been populated with R code to assign a function called my_pipe. The function has no code in the body so it won’t do anything just yet. Above the function declaration are some specially formatted code comments. These are maestro tags and they’re used to configure the scheduling and operation of the pipeline.2\nThe tags most important for scheduling are maestroFrequency and maestroStartTime. maestroFrequency is how often to run the pipeline. It takes a number and a time unit, like 1 day, 3 hours, 15 minutes, or 6 months. maestroStartTime is a Date (yyyy-mm-dd) or timestamp (yyyy-mm-dd HH:MM:SS) indicating when the schedule starts.\nConfiguring the start time is important if you have specific times you want it to run. If, for example, you want the pipeline to run once daily at 12:30, you’d use @maestroStartTime 2024-06-03 12:00:00 (note here that the date part doesn’t matter unless you schedule it in the future).\nWe won’t concern ourselves with the other tags for now; just know that there are more and they all have default values.\nNow let’s get the pipeline to do something. In the spirit of typical data engineering tasks, we’ll create an ETL (Extract, Transform, Load) pipeline that gets data from a source, transforms it by adding a new column, and loads it into storage.\n\n\nMaking a Useful Pipeline\nWe’ll use the open API from Environment Canada called Geomet for meteorological data and we’ll use DuckDB for storage. We’ll need the httr2 and duckdb packages for extraction and storage, respectively.\n\n#' Located in ./pipelines/my_pipe.R\n#' @maestroFrequency 1 day\n#' @maestroStartTime 2024-06-03\ngeomet_stations &lt;- function() {\n  \n  # Formulate the request  \n  req &lt;- httr2::request(\n    \"https://api.weather.gc.ca/collections/climate-stations/items\"\n  ) |&gt; \n    httr2::req_url_query(\n      limit = 1000,\n      skipGeometry = TRUE\n    )\n  \n  # Send the request and interpret the response\n  resp &lt;- req |&gt; \n    httr2::req_perform() |&gt; \n    httr2::resp_body_json(simplifyVector = TRUE)\n  \n  # Get the properties element where the rectangular data is located\n  stations_dat &lt;- resp$features$properties\n  \n  # Clean the names\n  stations_clean &lt;- stations_dat |&gt; \n    janitor::clean_names() |&gt; \n    janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n    dplyr::mutate(\n      insert_time = lubridate::now(tzone = \"UTC\")\n    )\n  \n  # Connect to a local in-memory duckdb\n  conn &lt;- DBI::dbConnect(duckdb::duckdb())\n  \n  # Create and write to a table\n  DBI::dbWriteTable(\n    conn, \n    name = \"geomet_stations_transactional\", \n    value = stations_clean\n  )\n  \n  # Test that it worked in the return\n  res &lt;- DBI::dbGetQuery(\n    conn, \n    statement = \"\n    select * from geomet_stations_transactional\n    order by insert_time desc\n    limit 10\n    \"\n  ) |&gt;\n    dplyr::as_tibble()\n  \n  DBI::dbDisconnect(conn)\n  \n  return(\n    res\n  )\n}\n\n\n\nOrchestrate It\nNow that we have a single useful pipeline, let’s orchestrate it (in practice, we’d probably have more than one pipeline). We’ll set the orchestrator to run at a daily frequency (this does not actually cause it to run daily, we need something else external to the R project to actually run it). For testing purposes, we’ll then run this interactively:\n\n# Located in ./orchestrator.R\nlibrary(maestro)\n\nschedule &lt;- build_schedule()\n\norch_result &lt;- run_schedule(\n  schedule,\n  orch_frequency = \"1 day\"\n)\n\nℹ 1 script successfully parsed                                                  \n                                                                                \n── [2024-09-23 10:29:02]                                                        \nRunning pipelines ▶                                                             \n✔ geomet_stations [566ms]                                                       \n                                                                                \n── [2024-09-23 10:29:02]                                                        \nPipeline execution completed ■ | 0.57 sec elapsed                               \n✔ 1 success | → 0 skipped | ! 0 warnings | ✖ 0 errors | ◼ 1 total               \n────────────────────────────────────────────────────────────────────────────────\n                                                                                \n── Next scheduled pipelines ❯                                                   \nPipe name | Next scheduled run                                                  \n• geomet_stations | 2024-09-25"
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html#deployment",
    "href": "posts/hello-maestro/hello-maestro.html#deployment",
    "title": "Hello maestro",
    "section": "Deployment",
    "text": "Deployment\nOk, so we ran the orchestrator interactively, but this is only useful for testing. In practice, we want to deploy this on a server and have it run every day. To be clear: maestro does not do this for you - it just assumes that you are doing this and behaves accordingly. In other words, when you declare orch_frequency = \"1 day\" you are saying that you intend to run the orchestrator every 1 day.\nThe first decision to make about deployment is local vs. cloud. If you own the server it’s local3, if you rent the hardware and connect to it remotely it’s cloud. Here, we’ll run through a straightforward local deployment because it requires less configuration and won’t cost you anything.\n\nMac/Linux: cronR\nCron is a job scheduler for the Mac/Linux systems. You can use cronR to interface with it via R.\n\nlibrary(cronR)\n\ncmd &lt;- cron_rscript(\n  \"orchestrator.R\", \n  workdir = getwd()\n)\n\ncron_add(\n  cmd, \n  frequency = \"daily\",\n  id = \"maestro\",\n  ask = FALSE\n)\n\n\n\nWindows: taskscheduleR\nWindows users can use taskscheduleR to schedule a job via R:\n\nlibrary(taskscheduleR)\n\ntaskscheduler_create(\n  taskname = \"maestro\", \n  rscript = \"orchestrator.R\", \n  schedule = \"DAILY\",\n  exec_path = getwd()\n)"
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html#final-remarks",
    "href": "posts/hello-maestro/hello-maestro.html#final-remarks",
    "title": "Hello maestro",
    "section": "Final Remarks",
    "text": "Final Remarks\nIn this post, we saw how maestro can be used to orchestrate multiple pipelines in a single R project. Maestro is in its early development, but I encourage you to try it out. We’re using it in production to orchestrate 18 production pipelines at the Halifax International Airport!\nI’d be remiss not to mention a few caveats of maestro:\n\nIt should only be used for light-medium scale batch jobs. If you need to do streaming and/or heavy load jobs, it’s probably not for you.\nConfiguring the schedule for the orchestrator requires some thought. If you have several pipelines at different intervals and times you’ll want to choose a frequency that makes sense. You can use suggest_orch_frequency() to get a heuristic suggestion based on a schedule. Even then - you need to make sure you actually run the orchestrator at this frequency. Weird things happen if your orchestrator actually runs more or less frequently than you said it would.\n\nThat said, I think maestro is great for small-medium sized pipeline orchestration. If you’re looking to deploy maestro on the cloud, this blog post will help you get started in Google Compute Platform (GCP).\n\nNotes\nThis post was created using R version 4.4.1 (2024-06-14) and maestro version 0.3.0."
  },
  {
    "objectID": "posts/hello-maestro/hello-maestro.html#footnotes",
    "href": "posts/hello-maestro/hello-maestro.html#footnotes",
    "title": "Hello maestro",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA popular R package for data pipelines that has been around for some time is targets. Maestro is different from targets in that the focus of maestro is to orchestrate multiple independent data pipelines whereas targets is for a single connected pipeline. Maestro would be more suited toward ETL data engineering where targets is for complex analytics pipelines.↩︎\nR users will recognize that maestro uses custom roxygen2 tags.↩︎\nYes, this includes your personal laptop or desktop; however, it probably goes to sleep when not used. If you’re using a Mac, you can use pmset in the command line to get around this (https://www.dssw.co.uk/reference/pmset/).↩︎"
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "",
    "text": "In the previous post we saw how maestro helps orchestrate data pipelines in a single R project. For maestro to be useful you need to deploy it somewhere and these days that ‘somewhere’ is often cloud-based. Here we’ll walk through deploying a maestro project in the cloud on Google Cloud Platform (GCP).1\nThis post assumes some familiarity with the maestro package although the practice of deploying on GCP can be generalized to almost any project. The previous blog post walks through maestro specifically.\nThis project is available on Github here."
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#design",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#design",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Design",
    "text": "Design\nLike any cloud offering, GCP gives us many different ways to solve the same problem.2 My suggested design is by no means the only way nor is it necessarily the best way. I strove for simplicity. In the end, I landed on 3 GCP services needed to deploy the project:\n\nCloud Build for hosting the containerized project.\nCloud Run Jobs for executing the container.\nCloud Scheduler for scheduling the container to run.\n\nThe most complicated of these is the first step and requires some familiarity with Docker (and ideally Github)."
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#dockerizing-maestro",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#dockerizing-maestro",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Dockerizing Maestro",
    "text": "Dockerizing Maestro\nThe best way to ensure that our maestro project runs in the cloud with all its dependencies is to use Docker. Docker is widespread in software development these days and is practically a requirement for deploying anything. I won’t go over the fundamentals of Docker here (Alex Gold’s DevOps for Data Science has a Demystifying Docker chapter that is very helpful if you’re new or rusty with Docker).\nWe need to create a Dockerfile at the root level of the project. I used dockerfiler for this. It has a function dock_from_renv() and it got me about 90% of the way there. As implied in the name you need to be using renv.3\nAfter using dockerfiler to generate the Dockerfile, I made some necessary tweaks. The main thing is to add a few system libraries and to add the ENTRYPOINT [\"Rscript\", \"orchestrator.R\"] so that Google Cloud Run knows to execute that script:\n# Dockerfile\nFROM rocker/r-ver\nRUN apt-get update && apt-get install -y \\\n    libxml2-dev \\\n    libcurl4-openssl-dev \\\n    libssl-dev \\\n    libsodium-dev \\\n    zlib1g-dev \\\n    pkg-config \\\n    && rm -rf /var/lib/apt/lists/*\nRUN R -e 'install.packages(\"remotes\")'\nRUN R -e 'remotes::install_version(\"renv\", version = \"1.0.7\")'\nWORKDIR /usr/src/app\nCOPY . .\nRUN R -e 'renv::restore()'\nENTRYPOINT [\"Rscript\", \"orchestrator.R\"]\nI also had to add some system libraries (libcurl4-openssl-dev for making the API requests). This part is a bit tedious and will depend on your situation. For instance, if you’re doing anything geospatial you may need lib-gdal. ChatGPT was pretty helpful when I ran into errors building my docker image.\nThis leads us to the next step: building the image and testing it in a container locally. This is where you can debug problems before you go to deploy to GCP.\n\ndocker build -t maestro_gcp .\ndocker run maestro_gcp --rm"
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#creating-a-cloudbuild.yaml",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#creating-a-cloudbuild.yaml",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Creating a cloudbuild.yaml",
    "text": "Creating a cloudbuild.yaml\nThe cloudbuild.yaml is a configuration file for telling GCP what image to use and how to build it. Think of it as the link connecting your project to the place where the docker image will live in GCP. Full confession: ChatGPT helped me with this one:\nsteps:\n- name: 'gcr.io/cloud-builders/docker'\n  args: ['build', '-t', 'gcr.io/$PROJECT_ID/maestro-gcp', '.']\nimages:\n- 'gcr.io/$PROJECT_ID/maestro-gcp'\noptions:\n  logging: CLOUD_LOGGING_ONLY\nName this file cloudbuild.yaml and add it to the root level of your project. At this point, you should have a project structure that looks something like this:\n.\n├── Dockerfile\n├── cloudbuild.yaml\n├── maestro-gcp-deploy.Rproj\n├── orchestrator.R\n├── pipelines\n├── renv\n└── renv.lock"
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#hosting-the-container-on-gcp",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#hosting-the-container-on-gcp",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Hosting the Container on GCP",
    "text": "Hosting the Container on GCP\nThe great thing about Cloud Build is you can host a project with a Dockerfile on Github and deploy it to GCP from there.\nCreate the Github repo and push the project code there. Then, in the GCP console and, inside of Cloud Build &gt; Triggers, select Connect Repository.\nOnce you open the Connect Repository wizard and select Github, it’ll step you through some authentication.\n\n\n\nSuccessful Cloud Build in GCP Console."
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#create-cloud-job-run",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#create-cloud-job-run",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Create Cloud Job Run",
    "text": "Create Cloud Job Run\nIf your build was successful, congrats! You made it through the hardest part. We now go to Cloud Run Jobs.4 Go to Create Job and browse through to find your recently build container.\n\n\n\nCreate Job wizard in GCP Console.\n\n\nYou may also want to select the box to ‘Execute job immediately’ to test that the container runs as expected."
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#schedule-it",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#schedule-it",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Schedule It",
    "text": "Schedule It\nWe can stay right within the Cloud Run service to do the scheduling even though it is handled by Google Cloud Scheduler.\nClick on your newly created job and then on Triggers &gt; Add Scheduler Trigger:\n\n\n\nAdding a scheduler trigger to the cloud job.\n\n\nThe Frequency input uses cron syntax. You can use Crontab to help specify the schedule. When it’s created you should see something below like this:\n\n\n\nSuccessfully created cron schedule for Cloud Run Job.\n\n\nI like to trigger the job manually too to see how it runs, or you can just wait until the schedule kicks in.\n\n\n\nLogs of the cloud run job."
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#final-remarks",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#final-remarks",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Final Remarks",
    "text": "Final Remarks\nAnd that about sums it up for the deployment. Cloud deployment is almost never a straightforward process, so be prepared for some failed attempts. I relied on ChatGPT when I ran into problems. My organization gives us access to ChatGPT 4o and it seems to have an excellent grasp of GCP. It tends to provide command line instructions rather than point-click in the GCP console, but this is for your benefit - it’s more reproducible and stable."
  },
  {
    "objectID": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#footnotes",
    "href": "posts/maestro-gcp-deployment/maestro-gcp-deployment.html#footnotes",
    "title": "Deploying a Maestro Project to Google Cloud Platform",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhy GCP and not AWS or Azure? We chose GCP for this initial post on deployment for two reasons. First is it has always free tiers for commonly used services. For lightweight use cases, you won’t have to worry about incurring a cloud bill. Another reason is I’ve found the learning curve for GCP much gentler compared to AWS and Azure. In general, GCP is much better at helping you do what you need to do at very little to no cost (for lightweight jobs).↩︎\nCloud providers are great at packaging the same fundamental activity in a million different ways. Try not to get overwhelmed with all the different services in GCP. Usually you’ll only ever make use of a few of the most common ones. Everything comes down to compute and storage.↩︎\nYes, I realize this may feel like a lot before we’ve even made it to GCP. While Docker and renv create additional work (and often frustration) up front, it truly is more beneficial when it comes to the actual deployment. Without Docker, setting up the runtime is an especially painful game of whack-a-mole. This doesn’t mean you won’t have to whack a few moles first though.↩︎\nMake sure you choose Create Job and not Create Service. Services listen for HTTP requests whereas jobs can be triggered by a cron schedule.↩︎"
  },
  {
    "objectID": "posts/data-contamination/main.html#data-decontamination-mindset",
    "href": "posts/data-contamination/main.html#data-decontamination-mindset",
    "title": "The Data Decontamination Mindset",
    "section": "Data Decontamination Mindset",
    "text": "Data Decontamination Mindset\nI’m suggesting to adopt a data decontamination mindset: immediately assume that data is contaminated. I think it is especially important to treat data coming from individuals as contaminated, particularly if the business doesn’t have strong data literacy. We usually can have a bit more trust in data coming from a reputable third-party API, but we should also apply a similar critical attitude in this case too.\nContaminated, poor quality data can have massive impacts downstream, so taking the time to decontaminate your data is critical. Don’t make the mistake I did and blindly trust that it’s all good because it’s coming from a domain expert. In the end, you should always assume the worst but hope for the best."
  },
  {
    "objectID": "posts/maestro-0-2-0/maestro-0-2-0.html",
    "href": "posts/maestro-0-2-0/maestro-0-2-0.html",
    "title": "maestro 0.2.0",
    "section": "",
    "text": "maestro 0.2.0 brings with it new features for scheduling pipelines on specific hours, days, or months. With a few new tags, we have so much more versatility for scheduling.\nIf you haven’t heard of maestro, it’s a package that helps you schedule your R scripts all in a single project using tags. You can learn more about it here.\nGet it from CRAN:\ninstall.packages(\"maestro\")"
  },
  {
    "objectID": "posts/maestro-0-2-0/maestro-0-2-0.html#new-maestrofrequency-syntax",
    "href": "posts/maestro-0-2-0/maestro-0-2-0.html#new-maestrofrequency-syntax",
    "title": "maestro 0.2.0",
    "section": "New maestroFrequency Syntax",
    "text": "New maestroFrequency Syntax\nThe maestroFrequency tag used to be restrictive in how you specified your pipeline frequency, only accepting it in the form of [n] [units] like 1 day, 4 hours, 2 weeks, etc. A more human-readable adverb option is now available. You can specify it as one of hourly, daily, weekly, biweekly, monthly, quarterly and yearly. Each of these is the equivalent of 1 [unit]. So, hourly = 1 hour.\n\n#' Example hourly pipeline\n#' @maestroFrequency hourly\nmy_hourly_job &lt;- function() {\n  # job code ...\n}\n\nYou can also use this syntax in run_schedule:\n\nlibrary(maestro)\n\nrun_schedule(\n  example_schedule,\n  orch_frequency = \"hourly\"\n)\n\nThese frequencies are important not only for readability but they also combine with hour, day, and month specifiers for more bespoke scheduling."
  },
  {
    "objectID": "posts/maestro-0-2-0/maestro-0-2-0.html#new-hours-days-months-specifiers",
    "href": "posts/maestro-0-2-0/maestro-0-2-0.html#new-hours-days-months-specifiers",
    "title": "maestro 0.2.0",
    "section": "New Hours, Days, Months Specifiers",
    "text": "New Hours, Days, Months Specifiers\nUntil now you could only run pipelines on regular intervals. With 0.2.0 you can specify particular hours, days, and months you want the pipelines to run. This is useful if you want to do something like have a job run only on business hours or just on weekends. 3 new tags maestroHours, maestroDays, and maestroMonths are available.\n\nSpecific Hours\nLet’s say I have a pipeline I want to run during regular 9am-5pm business hours. I can use a maestroFrequency of hourly and specify the hours 9am through to 5pm:\n\n#' Example work hours pipeline\n#' @maestroFrequency hourly\n#' @maestroHours 9 10 11 12 13 14 15 16 17\nmy_work_hours_job &lt;- function() {\n  # job code ...\n}\n\nThis will be UTC hours by default. If I want my timezone to be where I live I can specify maestroTz America/Halifax.\n\n\nSpecific Days\nWe can specify either days of week or days of month. Days of week use abbreviated weekdays like Mon, Wed, Sat; whereas days of month use integers 1, 10, 15, etc.\nTaking the above example further, let’s have it run during business hours on weekdays:\n\n#' Example work hours pipeline\n#' @maestroFrequency hourly\n#' @maestroHours 9 10 11 12 13 14 15 16 17\n#' @maestroDays Mon Tue Wed Thu Fri\nmy_work_hours_job2 &lt;- function() {\n  # job code ...\n}\n\n\n\nSpecific Months\nTo specify the months use the integers 1-12. Let’s imagine that our pipeline only runs in March, June, October, and December:\n\n#' Example work hours pipeline\n#' @maestroFrequency hourly\n#' @maestroHours 9 10 11 12 13 14 15 16 17\n#' @maestroDays Mon Tue Wed Thu Fri\n#' @maestroMonths 3 6 10 12\nmy_work_hours_job3 &lt;- function() {\n  # job code ...\n}\n\nThese specifiers must be used with -ly frequencies like hourly and daily. The type of specifier used must be at least the same as the base frequency. For example, we can’t use maestroHours on a pipeline with a daily frequency."
  },
  {
    "objectID": "posts/maestro-0-2-0/maestro-0-2-0.html#improvements-to-suggest_orch_frequency",
    "href": "posts/maestro-0-2-0/maestro-0-2-0.html#improvements-to-suggest_orch_frequency",
    "title": "maestro 0.2.0",
    "section": "Improvements to suggest_orch_frequency",
    "text": "Improvements to suggest_orch_frequency\nThe function suggest_orch_frequency takes a schedule generated from build_schedule and suggests the likely optimal frequency for the orchestrator. Until recently, this function was pretty basic and just suggested twice the amount of the highest frequency pipeline in the project. This wouldn’t work out well if you had pipelines staggered on different hours.\nNow suggest_orch_frequency looks for the smallest interval of time between pipelines. It won’t consider hours, days, months specifiers though.\n\nlibrary(maestro)\n\nschedule &lt;- build_schedule()\n\nℹ 1 script successfully parsed\n\nsuggest_orch_frequency(schedule)\n\n[1] \"1 day\"\n\n\nNote that suggest_orch_frequency assumes that you want to run your pipelines exactly when you want them to - it won’t try to round to the nearest 15 minute or whatever.\nCheck out the release notes for more details on what’s new in version 0.2.0. If you find any bugs or want to suggest new features and improvements, please add them here or reach out to me on LinkedIn.\nHappy orchestrating!"
  },
  {
    "objectID": "posts/maestro-0-3-0/maestro-0-3-0.html",
    "href": "posts/maestro-0-3-0/maestro-0-3-0.html",
    "title": "maestro 0.3.0",
    "section": "",
    "text": "maestro 0.3.0 introduces principled methods for obtaining a schedule table, pipeline status, and artifacts (i.e., return values). This is now possible because of a major refactoring of the backend to use R6 classes for pipelines and schedules. This also introduced a few breaking changes, but on the whole, not a lot is different.\nIf you haven’t heard of maestro, it’s a package that helps you schedule your R scripts all in a single project using tags. You can learn more about it here.\nGet it from CRAN:\ninstall.packages(\"maestro\")"
  },
  {
    "objectID": "posts/maestro-0-3-0/maestro-0-3-0.html#a-schedule-is-now-a-maestroschedule",
    "href": "posts/maestro-0-3-0/maestro-0-3-0.html#a-schedule-is-now-a-maestroschedule",
    "title": "maestro 0.3.0",
    "section": "A schedule is now a <MaestroSchedule>",
    "text": "A schedule is now a &lt;MaestroSchedule&gt;\nSchedules are now a special class with attributes and methods. For most users, this won’t change how they use maestro, but it’ll make it much easier to extend maestro with new features in the future. Let’s see how a typical maestro scenario plays out now:\n\nlibrary(maestro)\n\n# Create a few dummy pipelines for demo purposes\ncreate_pipeline(\n  \"my_pipe\", open = FALSE, quiet = TRUE, overwrite = TRUE\n  )\ncreate_pipeline(\n  \"my_pipe2\", frequency = \"1 week\", open = FALSE, quiet = TRUE, overwrite = TRUE\n  )\ncreate_pipeline(\n  \"my_pipe3\", frequency = \"2 hours\", open = FALSE, quiet = TRUE, overwrite = TRUE\n  )\n\nschedule &lt;- build_schedule(quiet = TRUE)\n\noutput &lt;- run_schedule(\n  schedule,\n  orch_frequency = \"hourly\"\n)\n\n\n\n\n── [2024-09-23 14:31:34]\nRunning pipelines ▶ \n\n\nℹ my_pipe3\n\n\n✔ my_pipe3 [8ms]\n\n\n\n\n\n── [2024-09-23 14:31:34]\nPipeline execution completed ■ | 0.101 sec elapsed \n\n\n✔ 1 success | → 2 skipped | ! 0 warnings | ✖ 0 errors | ◼ 3 total\n\n\n────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Next scheduled pipelines ❯ \n\n\nPipe name | Next scheduled run\n\n\n• my_pipe3 | 2024-09-23 20:00:00\n\n\n• my_pipe | 2024-09-24\n\n\n• my_pipe2 | 2024-09-30\n\n\nSo far these are the same steps as before. The difference is now in how we interact with the schedule. Here, schedule is no longer a data.frame, it’s a &lt;MaestroSchedule&gt; R6 object. If we want to get the status of each of the pipelines, we can use the new get_status() function:\n\nget_status(schedule)\n\n# A tibble: 3 × 10\n  pipe_name script_path  invoked success pipeline_started    pipeline_ended     \n  &lt;chr&gt;     &lt;chr&gt;        &lt;lgl&gt;   &lt;lgl&gt;   &lt;dttm&gt;              &lt;dttm&gt;             \n1 my_pipe   ./pipelines… FALSE   FALSE   NA                  NA                 \n2 my_pipe2  ./pipelines… FALSE   FALSE   NA                  NA                 \n3 my_pipe3  ./pipelines… TRUE    TRUE    2024-09-23 17:31:34 2024-09-23 17:31:34\n# ℹ 4 more variables: errors &lt;int&gt;, warnings &lt;int&gt;, messages &lt;int&gt;,\n#   next_run &lt;dttm&gt;\n\n\nIf we just want the schedule, we can use get_schedule():\n\nget_schedule(schedule)\n\n# A tibble: 3 × 9\n  script_path      pipe_name frequency start_time          tz    skip  log_level\n  &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt;     &lt;dttm&gt;              &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt;    \n1 ./pipelines/my_… my_pipe   1 day     2024-09-23 00:00:00 UTC   FALSE INFO     \n2 ./pipelines/my_… my_pipe2  1 week    2024-09-23 00:00:00 UTC   FALSE INFO     \n3 ./pipelines/my_… my_pipe3  2 hours   2024-09-23 00:00:00 UTC   FALSE INFO     \n# ℹ 2 more variables: frequency_n &lt;int&gt;, frequency_unit &lt;chr&gt;\n\n\nNote that we didn’t have to assign a new object when we ran run_schedule(). The object schedule is updated to reflect the execution of the schedule.\nIf any pipelines have return values, we can access those as a named list using get_artifacts():\n\nget_artifacts(schedule)\n\nnamed list()"
  },
  {
    "objectID": "posts/maestro-0-3-0/maestro-0-3-0.html#invoking-a-pipeline",
    "href": "posts/maestro-0-3-0/maestro-0-3-0.html#invoking-a-pipeline",
    "title": "maestro 0.3.0",
    "section": "Invoking a pipeline",
    "text": "Invoking a pipeline\nWhen interactively working in a maestro project, it may sometimes be useful to manually trigger a pipeline. Now, if you’ve created a &lt;MaestroSchedule&gt; object in the environment, you can do this using invoke(). This will execute the pipeline regardless of whether it’s scheduled or not.\n\ninvoke(schedule, pipe_name = \"my_pipe\")\n\nℹ my_pipe\n\n\n✔ my_pipe [6ms]\n\n\n\n\n\nCheck out the release notes for more details on what’s new in version 0.3.0. If you find any bugs or want to suggest new features and improvements, please add them here or reach out to me on LinkedIn.\nHappy orchestrating!"
  },
  {
    "objectID": "posts/maestro-dags/main.html",
    "href": "posts/maestro-dags/main.html",
    "title": "Chain your maestro pipelines DAG style",
    "section": "",
    "text": "I’m thrilled to introduce the biggest maestro update yet. DAGs! A DAG (directed acyclic graph) in the context of data pipelines is when you have data processing steps that connect into other steps until a final step is reached. Almost all data orchestration platforms use the concept of DAGs to increase reusability and isolation of discrete components. As of maestro 0.4.0, DAGs are now possible using maestroInputs and maestroOutputs tags. This post will go through the motivation and implementation of this new feature.\nIf you haven’t heard of maestro, it’s a package that helps you schedule your R scripts all in a single project using tags. You can learn more about it here.\nGet it from CRAN:\ninstall.packages(\"maestro\")"
  },
  {
    "objectID": "posts/maestro-dags/main.html#why-dags",
    "href": "posts/maestro-dags/main.html#why-dags",
    "title": "Chain your maestro pipelines DAG style",
    "section": "Why DAGs?",
    "text": "Why DAGs?\nLet’s imagine we have a data pipeline where we want to extract data, clean/transform it, train a model, and send the predictions to a database. We can take each of these steps and chain them together so that the output of ‘extract’ is automatically fed into ‘clean/transform’, and so on.\nThe advantage of doing this in maestro is that you get better observability and retracability along each step. As we’ll see, we can more clearly identify where errors occur and even recover intermediate results."
  },
  {
    "objectID": "posts/maestro-dags/main.html#dags-in-maestro",
    "href": "posts/maestro-dags/main.html#dags-in-maestro",
    "title": "Chain your maestro pipelines DAG style",
    "section": "DAGs in maestro",
    "text": "DAGs in maestro\nIn short, a DAG pipeline is created using either maestroInputs or maestroOutputs tags. Both are valid but usually only one is needed. Simply put, a pipeline with a tag #' @maestroInputs start_pipe receives the output from a pipeline called start_pipe. Alternatively, we could use #' @maestroOutputs end_pipe to indicate that the pipeline called end_pipe receives the input of the current pipeline.\nLet’s see an example where we make model predictions on the nycflights13 data.\n\n#' /pipelines/model_flights.R\n#' @maestroFrequency daily\n#' @maestroStartTime 2024-11-22 09:00:00\n#' @maestroOutputs process_flights\nextract_flights &lt;- function() {\n  \n  # Imagine this is from a source where the data changes\n  nycflights13::flights\n}\n\n#' @maestroOutputs train_model\nprocess_flights &lt;- function(.input) {\n  \n  daily_flights &lt;- .input |&gt; \n    dplyr::mutate(date = lubridate::make_date(year, month, day)) |&gt; \n    dplyr::summarise(\n      n_flights = dplyr::n(), .by = date\n    )\n  \n  # A simple time series\n  ts(data = daily_flights$n_flights, frequency = 365)\n}\n\n#' @maestroOutputs forecast_flights\ntrain_model &lt;- function(.input) {\n  \n  # A simple ARIMA model (using the {forecast} package would be better)\n  .input |&gt; \n    arima(order = c(1, 1, 1))\n}\n\n#' @maestro\nforecast_flights &lt;- function(.input) {\n  \n  # Forecast the next 7 days\n  pred_obj &lt;- predict(.input, n.ahead = 7)\n  pred_obj$pred\n}\n\nWe won’t focus much on the content of the functions. Instead, pay attention to the use of maestroOutputs. Each function that outputs into another references the name of that function. The last function forecast_flights just uses a generic #' @maestro tag to indicate that it is part of the maestro project. Also note the use of the special keyword .input. This argument must be supplied to all functions receiving an input. Use this argument to capture the data being passed each step along the pipeline.\nNow we can build the schedule like always.\n\n# /orchestrator.R\nlibrary(maestro)\n\nschedule &lt;- build_schedule(quiet = TRUE)\n\nWe can verify that the DAG is properly defined using the show_network() function on our newly created schedule.\n\nshow_network(schedule)\n\n\n\n\n\nNow we can run the schedule. For testing purposes, we’ll set run_all = TRUE so that the pipeline runs no matter what the scheduling is.\n\nrun_schedule(\n  schedule,\n  run_all = TRUE\n)\n\n\n\n\n── [2024-11-22 14:25:11]\nRunning pipelines ▶ \n\n\nℹ extract_flights\n\n\n✔ extract_flights [768ms]\n\n\n\n\n\nℹ |-process_flights\n\n\n✔ |-process_flights [24ms]\n\n\n\n\n\nℹ   |-train_model\n\n\n✔   |-train_model [9ms]\n\n\n\n\n\nℹ   |-  |-forecast_flights\n\n\n✔   |-  |-forecast_flights [5ms]\n\n\n\n\n\n── [2024-11-22 14:25:11]\nPipeline execution completed ■ | 0.833 sec elapsed \n\n\n✔ 4 successes | → 0 skipped | ! 0 warnings | ✖ 0 errors | ◼ 4 total\n\n\n────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Maestro Schedule with 4 pipelines:  \n\n\n• Success\n\n\nWe can see from the console output that the whole pipeline ran successfully. If we want to get the output from each of the steps, we can use get_artifacts(). This returns intermediate results too, which can be helpful if you want to retrieve state after a failed run of the schedule.\n\nartifacts &lt;- get_artifacts(schedule)\nartifacts$forecast_flights\n\nTime Series:\nStart = c(2, 1) \nEnd = c(2, 7) \nFrequency = 365 \n[1] 942.7232 934.9722 933.7773 933.5931 933.5647 933.5603 933.5596\n\n\nMaestro can be used to create any valid DAG (e.g., branching, joining, etc.). I hope this new addition to maestro super charges your data orchestration.\nCheck out the release notes for more details on what’s new in version 0.4.0. If you find any bugs or want to suggest new features and improvements, please add them here or reach out to me on LinkedIn.\nHappy orchestrating!"
  },
  {
    "objectID": "posts/you-dont-need-airflow/main.html",
    "href": "posts/you-dont-need-airflow/main.html",
    "title": "You Don’t Need Airflow: Orchestrate Many Data Flows in R with Maestro",
    "section": "",
    "text": "Data never stays in one place for long. Any business or team that works with data needs to be thinking about how data moves from one place to the next. This often happens multiple times, continuously, and in multiple different streams. The concept of moving data is data flow1. When you have many data flows that need to be managed it’s called data orchestration. More specifically, data orchestration is the process of automating the ingestion, transformation, and analysis of data from multiple different locations and then making it widely accessible to users.\nIf you look at data orchestration tools today you are bombarded with a dizzying array of software platforms that claim unsurpassed processing capability, AI-readiness, elegant UIs, etc. Apache Airflow is just one example of a popular orchestration platform that scales to meet virtually any orchestration need. And while these claims may be true, I argue it is rarely the case that these gargantuan platforms are needed in the first place. For most data engineers, you probably only need to process a moderate amount of data at a moderate time scale. Moreover, if you’re an R user, you don’t want to have to define your data pipelines using drag-and-drop tools or learn another programming language. Not only will this reduce cloud costs but also development time costs.\nThis was the inspiration for maestro - an R package for orchestrating data jobs within a single project. Here I’ll demonstrate a maestro project and how the cost of deploying it likely compares to other data orchestration platforms currently available.\nIn this example, I’ll use open data from Cornell Lab’s eBird API providing free access to global bird observations and statistics. Note that a user account and API key are required to access the API."
  },
  {
    "objectID": "posts/you-dont-need-airflow/main.html#maestro",
    "href": "posts/you-dont-need-airflow/main.html#maestro",
    "title": "You Don’t Need Airflow: Orchestrate Many Data Flows in R with Maestro",
    "section": "Maestro",
    "text": "Maestro\n\n\n\n\n\n\nNote\n\n\n\nCheck out the maestro docs for a more detailed introduction to maestro.\n\n\nA maestro project consists of one or more pipelines (i.e., R functions with roxygen tags) and a single orchestrator script responsible for invoking the pipelines according to a schedule.\nThe project structure will look something like this:\nsample_project\n├── orchestrator.R\n└── pipelines\n    ├── get_nearby_notable_obs.R\n    ├── get_region_stats.R\n    └── get_species_list.R\n\nPipelines\nI’ve created three pipelines that each retrieve data from one of the eBird endpoints and stores it in a duckdb table. Each pipeline is scheduled to run at a particular time interval so that new data is regularly inserted into the table.\n\n\n\n\n\n\nTip\n\n\n\nThe #' @maestroFrequency is one of several tags that can be used to configure the scheduling of the pipeline. See here for more details.\n\n\n\nget_nearby_notable_obs.Rget_region_stats.Rget_species_list.R\n\n\n\n#' @maestroFrequency 3 hours\n#' @maestroStartTime 2025-02-20 12:00:00\n#' @maestroTz America/Halifax\nget_nearby_notable_obs &lt;- function() {\n\n  req &lt;- httr2::request(\"https://api.ebird.org/v2\") |&gt;\n    httr2::req_url_path_append(\"data/obs/geo/recent/notable\") |&gt;\n    httr2::req_url_query(\n      lat = 44.88,\n      lng = -63.52\n    ) |&gt;\n    httr2::req_headers(\n      `X-eBirdApiToken` = Sys.getenv(\"EBIRD_API_KEY\")\n    )\n\n  resp &lt;- req |&gt;\n    httr2::req_perform()\n\n  obs &lt;- resp |&gt;\n    httr2::resp_body_json(simplifyVector = TRUE) |&gt;\n    dplyr::mutate(\n      insert_time = Sys.time()\n    )\n\n  # Connect to a local in-memory duckdb\n  conn &lt;- DBI::dbConnect(duckdb::duckdb())\n  on.exit(DBI::dbDisconnect(conn))\n\n  # Create and write to a table\n  DBI::dbWriteTable(\n    conn,\n    name = \"recent_notable_observations\",\n    value = obs,\n    append = TRUE\n  )\n}\n\n\n\n\n#' @maestroFrequency 1 day\n#' @maestroStartTime 2025-02-20 18:00:00\n#' @maestroTz America/Halifax\nget_region_stats &lt;- function() {\n\n  now &lt;- Sys.time()\n  cur_year &lt;- lubridate::year(now)\n  cur_month &lt;- lubridate::month(now)\n  cur_day &lt;- lubridate::day(now)\n\n  req &lt;- httr2::request(\"https://api.ebird.org/v2\") |&gt;\n    httr2::req_url_path_append(\"product/stats\", \"CA-NS\", cur_year, cur_month, cur_day) |&gt;\n    httr2::req_headers(\n      `X-eBirdApiToken` = Sys.getenv(\"EBIRD_API_KEY\")\n    )\n\n  resp &lt;- req |&gt;\n    httr2::req_perform()\n\n  stats &lt;- resp |&gt;\n    httr2::resp_body_json(simplifyVector = TRUE) |&gt;\n    dplyr::as_tibble()\n\n  # Connect to a local in-memory duckdb\n  conn &lt;- DBI::dbConnect(duckdb::duckdb())\n  on.exit(DBI::dbDisconnect(conn))\n\n  # Create and write to a table\n  DBI::dbWriteTable(\n    conn,\n    name = \"region_stats\",\n    value = stats,\n    append = TRUE\n  )\n}\n\n\n\n\n#' @maestroFrequency 1 day\n#' @maestroStartTime 2025-02-20 15:00:00\n#' @maestroTz America/Halifax\nget_species_list &lt;- function() {\n\n  req &lt;- httr2::request(\"https://api.ebird.org/v2\") |&gt;\n    httr2::req_url_path_append(\"product/spplist\", \"CA-NS\") |&gt;\n    httr2::req_headers(\n      `X-eBirdApiToken` = Sys.getenv(\"EBIRD_API_KEY\")\n    )\n\n  resp &lt;- req |&gt;\n    httr2::req_perform()\n\n  spec_list &lt;- resp |&gt;\n    httr2::resp_body_json(simplifyVector = TRUE)\n\n  spec_df &lt;- dplyr::tibble(\n    speciesCode = spec_list\n  ) |&gt;\n    dplyr::mutate(\n      insert_time = Sys.time()\n    )\n\n  # Connect to a local in-memory duckdb\n  conn &lt;- DBI::dbConnect(duckdb::duckdb())\n  on.exit(DBI::dbDisconnect(conn))\n\n  # Create and write to a table\n  DBI::dbWriteTable(\n    conn,\n    name = \"species_list\",\n    value = spec_df,\n    append = TRUE\n  )\n}\n\n\n\n\n\n\nOrchestrator\nWith the pipelines created we move to the orchestrator script. This is an R script or Quarto document that runs maestro functions to create the schedule from the tags and the run the schedule according to some frequency - a frequency that should always be at least as frequent as your most frequent pipeline.\n\nlibrary(maestro)\n\nschedule &lt;- build_schedule()\n\nℹ 3 scripts successfully parsed\n\nrun_schedule(\n  schedule, \n  orch_frequency = \"1 hour\",\n  check_datetime = as.POSIXct(\"2025-02-26 15:00:00\", tz = \"America/Halifax\") # for reproducibility - in practice use Sys.time()\n)\n\n\n── [2025-02-26 15:14:23]\nRunning pipelines ▶ \nℹ get_nearby_notable_obs\n✔ get_nearby_notable_obs [758ms]\n\nℹ get_species_list\n✔ get_species_list [108ms]\n\n\n── [2025-02-26 15:14:24]\nPipeline execution completed ■ | 0.885 sec elapsed \n✔ 2 successes | → 1 skipped | ! 0 warnings | ✖ 0 errors | ◼ 3 total\n────────────────────────────────────────────────────────────────────────────────\n\n── Next scheduled pipelines ❯ \nPipe name | Next scheduled run\n• get_nearby_notable_obs | 2025-02-26 22:00:00\n• get_region_stats | 2025-02-26 22:00:00\n• get_species_list | 2025-02-27 19:00:00\n\n── Maestro Schedule with 3 pipelines:  \n• Success\n\n\n\nstatus &lt;- get_status(schedule)\nstatus\n\n# A tibble: 3 × 10\n  pipe_name  script_path invoked success pipeline_started    pipeline_ended     \n  &lt;chr&gt;      &lt;chr&gt;       &lt;lgl&gt;   &lt;lgl&gt;   &lt;dttm&gt;              &lt;dttm&gt;             \n1 get_nearb… ./pipeline… TRUE    TRUE    2025-02-26 19:14:23 2025-02-26 19:14:24\n2 get_regio… ./pipeline… FALSE   FALSE   NA                  NA                 \n3 get_speci… ./pipeline… TRUE    TRUE    2025-02-26 19:14:24 2025-02-26 19:14:24\n# ℹ 4 more variables: errors &lt;int&gt;, warnings &lt;int&gt;, messages &lt;int&gt;,\n#   next_run &lt;dttm&gt;\n\n\nWe can run all this interactively, but the power of maestro is in running it scheduled in production. This way, the data will grow and update regularly. Deployment is not special in the case of maestro - you just need to be sure that the orchestrator is scheduled to run at the same frequency as specified in orch_frequency. Check out my previous post for a walk through of deployment on Google Cloud."
  },
  {
    "objectID": "posts/you-dont-need-airflow/main.html#monitoring",
    "href": "posts/you-dont-need-airflow/main.html#monitoring",
    "title": "You Don’t Need Airflow: Orchestrate Many Data Flows in R with Maestro",
    "section": "Monitoring",
    "text": "Monitoring\nIn production it is essential to monitor the status of data flows so that issues can be identified and resolved. There are a few extra steps to set this up for maestro:\n\nStore results of get_status() in a separate table.\nCreate and host a visualization/dashboard with the pipeline statuses.\n\nStep 1 will involve adding a few lines of code in the orchestrator script. In our example using duckdb, it looks like this:\n\nstatus &lt;- get_status(schedule)\n\nconn &lt;- DBI::dbConnect(duckdb::duckdb())\n\nDBI::dbWriteTable(\n  conn,\n  name = \"maestro_status\",\n  value = status,\n  append = TRUE\n)\n\nDBI::dbDisconnect(conn)\n\nHere, I’ll simulate multiple runs of the orchestrator to make it seem like it had been running for a few days. In practice, you would just read the table containing the pipeline statuses.\n\n\nShow the code\nset.seed(233)\nn_runs &lt;- 3 * 24\nlast_run &lt;- as.POSIXct(\"2025-02-26 15:00:00\", tz = \"America/Halifax\")\nrun_seq &lt;- last_run - lubridate::hours(0:n_runs)\n\n# This leverages the lower-level MaestroPipeline class. This is almost never needed in practice\nstatus_extended_ls &lt;- purrr::map(schedule$PipelineList$MaestroPipelines, \\(x) {\n  purrr::map(run_seq, \\(y) {\n    pipe_name &lt;- x$get_pipe_name()\n    run_pipe &lt;- x$check_timeliness(orch_n = 1, orch_unit = \"hour\", check_datetime = y)\n    if (run_pipe) {\n      dplyr::tibble(\n        pipe_name = pipe_name,\n        invoked = TRUE,\n        success = sample(c(TRUE, FALSE), 1, prob = c(0.8, 0.2)),\n        pipeline_started = y,\n        pipeline_ended = pipeline_started + lubridate::seconds(sample(seq(0.4, 5, by = 0.05), 1)),\n      )\n    } else {\n      dplyr::tibble(\n        pipe_name = pipe_name,\n        invoked = FALSE,\n        success = FALSE,\n        pipeline_started = NA,\n        pipeline_ended = NA\n      )\n    }\n  }) |&gt; \n    purrr::list_rbind()\n})\n\nstatus_extended_df &lt;- purrr::list_rbind(status_extended_ls)\n\n\nThere are many ways to visualize the status of pipelines. If the number of pipelines is small and your time window is similarly small you can get away with a static ggplot. The code below uses the simulated status data.frame to generate a simple Gantt chart where green indicates success and red failure.\n\n\nShow the code\nlibrary(ggplot2)\n\nstatus_extended_df |&gt; \n  ggplot(aes(x = pipeline_started, y = 0)) +\n  geom_segment(aes(xend = pipeline_ended, yend = 1, color = success)) +\n  scale_color_manual(values = c(\"firebrick2\", \"seagreen3\")) +\n  facet_wrap(~pipe_name, ncol = 1) +\n  labs(\n    y = NULL,\n    x = NULL,\n    title = \"Pipeline Status\"\n  ) +\n  guides(color = \"none\") +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nAs the number of pipelines grows and/or you want more flexibility around time frame, you may want to build a dashboard with interactive figures. The below image is a screenshot of the dashboard I built using shiny at Halifax Stanfield International Airport that monitors the ~20 production pipelines in our environment.\n\n\n\nPipeline monitoring dashboard used in production at Halifax Stanfield International Airport\n\n\nIt’s not hard to imagine a future extension package that creates these sorts of monitoring dashboards automatically.\nNow that we’ve seen how maestro works, let’s look at why we might want to use maestro over something like Airflow."
  },
  {
    "objectID": "posts/you-dont-need-airflow/main.html#maestro-reduces-cost",
    "href": "posts/you-dont-need-airflow/main.html#maestro-reduces-cost",
    "title": "You Don’t Need Airflow: Orchestrate Many Data Flows in R with Maestro",
    "section": "Maestro Reduces Cost",
    "text": "Maestro Reduces Cost\nIf you’re an R developer, the answer to the question Why use Maestro is obvious: because I don’t need to use another language. However, there are other reasons for preferring maestro over enterprise orchestration software like Airflow or Mage, chief among these being cost.2\nThere are two primary reasons why maestro saves on cloud bills:\n\nMaestro is serverless - it can scale down to zero when the orchestrator is not running. In other words, if the orchestrator frequency isn’t too high (~15 minutes or more) you don’t need to run it on a continuously available server. Something like AWS Lambda, Google Cloud Run, or Azure Functions would work just fine.\nMaestro bundles multiple flows into a single instance. Assuming the number of flows and their frequencies doesn’t exceed limits you can run a moderate enterprise entirely within a single instance. No need to spin up and schedule separate instances for each data flow.3\n\nLet’s compare a few scenarios for cost estimates. In all cases, we’ll imagine we have 10 production data flows that run at various intervals ranging from every hour to every day. The scenarios are:\n\nMaestro running serverless every 1 hour\nSeparate scripts running serverless on separate instances\nRunning an Airflow project open-source in the cloud\nOrchestration platform provided by the cloud provider\n\n\n\n\n\n\n\nNote\n\n\n\nThese are back-of-the-napkin estimates based on conversations with ChatGPT and cloud computing documentation. Do not use these estimates as the sole basis for determining which tech stack will be more affordable. If I have made egregious errors in my estimates, please reach out to me via LinkedIn.\n\n\nI asked ChatGPT to provide estimates not specific to any one cloud provider (see appendix ChatGPT Conversation for conversation). The monthly costs in CAD are listed below:\n\nMaestro-style serverless: $25-35 ($35-45 if using multi-core)\nSeparate scheduled serverless scripts: $110-130\nAirflow: $170–200\nCloud managed: $80–100\n\nThis suggests a substantial cost savings for using a maestro-style architecture. Please note that these are estimates and are not substantiated by any experimentation. It’s worth considering that the costs appear to take into account storage but probably don’t account for image hosting, CI/CD, out-of-the-box monitoring, etc. that would likely come with fully featured orchestration platforms."
  },
  {
    "objectID": "posts/you-dont-need-airflow/main.html#maestro-eases-configuration-and-bolsters-metadata",
    "href": "posts/you-dont-need-airflow/main.html#maestro-eases-configuration-and-bolsters-metadata",
    "title": "You Don’t Need Airflow: Orchestrate Many Data Flows in R with Maestro",
    "section": "Maestro Eases Configuration and Bolsters Metadata",
    "text": "Maestro Eases Configuration and Bolsters Metadata\nOne of the challenges of orchestrating multiple data flows is keeping track of scheduling. Maestro eases this burden by requiring the scheduling configuration to be exactly where the pipeline is. This is not a new concept (Dagster uses decorators for scheduling) but it is rare to find in other platforms.4 This also follows the practice of infrastructure-as-code which makes projects more portable and reproducible.\nI’m also discovering a new advantage to declaring pipeline configuration with the pipeline code itself, and that is it makes it more AI-interpretable. In my own environment at the airport, I’m looking for ways to reduce and even eliminate manual effort to document tables and processes. In our informal explorations, we’ve found that giving an LLM sample data and pipeline code is enough to populate almost all the critical metadata around table descriptions, column descriptions, and update frequency even for domain-specific processes. I’m hopeful that this will greatly cutback on the amount of manual documentation and metadata entry that no one wants to do anyway."
  },
  {
    "objectID": "posts/you-dont-need-airflow/main.html#conclusions",
    "href": "posts/you-dont-need-airflow/main.html#conclusions",
    "title": "You Don’t Need Airflow: Orchestrate Many Data Flows in R with Maestro",
    "section": "Conclusions",
    "text": "Conclusions\nHere we saw how a single, cost-efficient maestro project can meet data orchestration needs for a moderate enterprise. Too often, I see people arguing on LinkedIn that if you’re a data engineer you need to use Airflow, or Spark, or [insert next trendy orchestration platform here]. The people who say this are usually trying to inflate their ego, sell you something, or they work for companies processing terabytes of data a minute and thus genuinely need a beastly platform. Most organizations don’t need that kind of power. In the end, it’s important to right-size the hardware and compute resources for the actual compute needs of your project. At the very least, if you’re an R user facing the conundrum of how to orchestrate your scripts, maestro is a great way to start familiar, start small, but with the potential to scale up to meet most of your needs.\nThis post was created using R version 4.4.2 (2024-10-31) and maestro version 0.5.1."
  },
  {
    "objectID": "posts/you-dont-need-airflow/main.html#appendices",
    "href": "posts/you-dont-need-airflow/main.html#appendices",
    "title": "You Don’t Need Airflow: Orchestrate Many Data Flows in R with Maestro",
    "section": "Appendices",
    "text": "Appendices\n\nChatGPT Conversation\nThe conversation below was with ChatGPT 4-o on February 26th 2025.\nMe: I want to do some cost estimates for cloud computing. I have three proposed tech stacks, all of which I could run on any cloud provider. I don’t want to compare cloud providers but instead compare the estimated cost of each architecture. In all cases, the project would involve taking 10 data flows that process around 500MB-2GB of data at varying time intervals with the most frequent being every hour and the least being every day. These data flows could be Python or R scripts or something else depending on the tech stack.\nHere are the three proposed tech stacks:\n\nAll 10 data flows run on a single instance and an orchestrator script runs every hour to determine which script to run. Importantly, all of this runs on a single serverless instance.\nEach 10 data flow has its own serverless instance and a scheduling service from the cloud provider is used to configure each script to run at its required time interval.\nAn open-source Apache Airflow project is hosted on a compute instance.\nAn orchestration platform provided by the cloud provider is used out of the box.\n\nFor each scenario, provide some estimate as to the hardware requirements (e.g., instance size, compute nodes, etc.) and give a rough estimate of monthly cost in CAD.\n\nI’ll break down the estimated cloud costs for each of the four approaches based on typical cloud pricing. Since we’re not comparing cloud providers directly, I’ll use common pricing models across AWS, GCP, and Azure. Prices will be in CAD.\n\nApproach 1: Single Serverless Instance for All Data Flows\n\nHardware Requirements:\n\nOne serverless instance that scales based on workload.\nNeeds at least 2 vCPUs and 4GB RAM, but scales up if a heavy script runs.\n~2GB of storage for script dependencies.\nAssumes processing 500MB-2GB per run.\n\nCost Breakdown:\n\nCompute: $0.000016 per second per vCPU ($0.06 per vCPU-hour)\nMemory: $0.0000025 per second per GB ($0.009 per GB-hour)\nAssuming each flow runs for an average of 5 minutes and scales up to 4 vCPUs and 8GB RAM occasionally:\n\n10 flows × 24 runs/day × 30 days = 7,200 runs/month\nAvg runtime: 5 min per run = 36,000 minutes per month = 600 hours\nAt $0.06 per vCPU-hour, 2 vCPUs cost $72/month.\nAt $0.009 per GB-hour, 4GB RAM costs $21.6/month.\n\nStorage & Data Transfer: Minimal, say $5/month.\n\nEstimated Cost: $100–120 CAD/month\n\n\nApproach 2: Separate Serverless Instance for Each Data Flow\n\nHardware Requirements:\n\nEach of the 10 data flows has its own serverless function.\nSimilar compute and memory needs as Approach 1 but scaled per function.\nA cloud scheduling service (e.g., AWS EventBridge, Azure Logic Apps, or GCP Cloud Scheduler) triggers them.\n\nCost Breakdown:\n\nCompute per function: Assume same 5 minutes per run.\n10 functions × 24 runs/day × 30 days = 7,200 total executions.\nIf each function scales to 2 vCPUs and 4GB RAM per run:\n\nAt $0.06 per vCPU-hour, cost per function = $7.2/month × 10 = $72/month.\nAt $0.009 per GB-hour, cost per function = $2.16/month × 10 = $21.6/month.\n\nScheduling service: ~$5/month.\nStorage & Data Transfer: ~$10/month.\n\nEstimated Cost: $110–130 CAD/month\n\n\nApproach 3: Apache Airflow on a Compute Instance\n\nHardware Requirements:\n\n1 compute instance for Airflow scheduler and web server: 2 vCPUs, 4GB RAM.\n1 compute instance for Airflow worker: 4 vCPUs, 8GB RAM.\n10 tasks run dynamically on the worker.\n\nCost Breakdown:\n\nCompute:\n\nScheduler instance: $50–60/month.\nWorker instance: $80–100/month.\n\nStorage & Networking: ~$10/month.\nManaged Database (PostgreSQL for metadata): ~$30/month.\n\nEstimated Cost: $170–200 CAD/month\n\n\nApproach 4: Cloud Provider’s Managed Orchestration Service\n\nHardware Requirements:\n\nFully managed service.\nUnderlying compute managed by the provider (often charged per task execution).\nAssumes a pay-per-task execution model.\n\nCost Breakdown:\n\nTask Execution Pricing: ~$0.0025–$0.005 per task execution.\n7,200 runs × $0.003 = $22/month.\nCompute Costs: (~$50/month for light usage, scales if workflows are complex).\nStorage & API Calls: ~$5–10/month.\n\nEstimated Cost: $80–100 CAD/month\n\n\nSummary of Estimated Monthly Costs (CAD)\n\n\n\n\n\n\n\n\nApproach\nEstimated Monthly Cost (CAD)\nKey Considerations\n\n\n\n\n1. Single Serverless Instance\n$100–120\nSimple but limited scalability.\n\n\n2. Separate Serverless Instances\n$110–130\nMore parallelism but increased function calls.\n\n\n3. Apache Airflow on Compute\n$170–200\nMore control, but higher infra cost.\n\n\n4. Cloud Managed Orchestration\n$80–100\nScalable and cost-efficient.\n\n\n\n\nRecommendation\n\nIf cost is the main factor → Approach 4 (Managed Orchestration) is best.\nIf you prefer full control → Approach 3 (Airflow).\nIf you need balance between control and scalability → Approach 2 (Separate Serverless Instances).\n\n\nMe: In the first scenario are you properly accounting for the fact that the job would only run once every hour? Even if there are multiple processes being executed, it is still one job scheduled to run hourly.\n\nYou’re absolutely right! In Scenario 1 (Single Serverless Instance Running an Orchestrator Script Hourly), the orchestrator script only runs once per hour, making the cost lower than I initially estimated. Let’s correct that calculation.\n\nScenario 1: Revised Cost Calculation\n\nExecution Frequency:\n\nThe orchestrator runs once every hour → 24 runs per day.\nEach run executes one or more data flows sequentially.\nAssume an average runtime of 10 minutes per execution (to process multiple flows).\n\nCompute Usage:\n\nSuppose the instance scales between 2–4 vCPUs and 4–8GB RAM, depending on the workload.\nAt peak, assume 2 vCPUs and 4GB RAM for the entire execution.\nCost estimation:\n\n24 runs/day × 30 days = 720 runs/month.\n10 minutes per run = 7,200 minutes per month = 120 compute hours/month.\nCompute cost: $0.06 per vCPU-hour × 2 vCPUs × 120 hours = $14.40/month.\nMemory cost: $0.009 per GB-hour × 4GB × 120 hours = $4.32/month.\n\n\nStorage & Data Transfer:\n\nStorage: ~$5/month.\nData transfer: ~$5/month.\n\n\nTotal Estimated Cost: $25–35 CAD/month\n\nUpdated Cost Comparison (CAD)\n\n\n\n\n\n\n\n\nApproach\nUpdated Monthly Cost (CAD)\nKey Considerations\n\n\n\n\n1. Single Serverless Instance\n$25–35 ✅\nMost cost-effective but limited parallelism.\n\n\n2. Separate Serverless Instances\n$110–130\nMore parallelism but higher function call costs.\n\n\n3. Apache Airflow on Compute\n$170–200\nFull control but highest infra cost.\n\n\n4. Cloud Managed Orchestration\n$80–100\nScalable and cost-efficient.\n\n\n\n\nFinal Thoughts\n\nScenario 1 is now clearly the cheapest option (~$25–35 CAD/month).\nHowever, it lacks parallelism—if multiple heavy processes need to run, the single instance might take longer to complete.\nIf cost is the priority, Scenario 1 is the best choice.\nIf scalability and efficiency matter, Scenario 4 (Managed Orchestration) is a good balance.\n\nWould you like a cost efficiency vs. performance trade-off chart to visualize these differences? 🚀\n\nMe: Finally, let’s say for Scenario 1 that we enable parallel processing if needed. For example, if 4 processes need to run simultaneously it can scale up to 4 cores.\n\nGreat! If we allow Scenario 1 to scale dynamically to support parallel processing, we need to adjust the compute cost based on variable CPU utilization. Here’s how that impacts the estimation.\n\nScenario 1: Revised Cost with Parallel Processing\nAssumptions:\n\nOrchestrator runs every hour (24 times per day).\nEach execution lasts 10 minutes but now can use up to 4 vCPUs when needed.\nAverage vCPU usage per run:\n\nSome runs may use 1–2 vCPUs (light workloads).\nOthers may spike to 4 vCPUs (heavy parallel processing).\nAssume an average usage of 3 vCPUs per execution.\n\nMemory scales accordingly: Assume 6GB RAM on average.\n\nCompute Cost Calculation:\n\nTotal monthly runtime:\n\n10 min per execution × 24 runs/day × 30 days = 7,200 minutes (~120 hours/month).\n\nCPU Cost:\n\nAvg 3 vCPUs per run.\n$0.06 per vCPU-hour × 3 vCPUs × 120 hours = $21.60/month.\n\nMemory Cost:\n\nAvg 6GB RAM per run.\n$0.009 per GB-hour × 6GB × 120 hours = $6.48/month.\n\nStorage & Data Transfer:\n\n$5–10/month (no significant change).\n\n\nFinal Total Cost: ~$35–45 CAD/month\n\nFinal Cost Comparison (CAD)\n\n\n\n\n\n\n\n\nApproach\nMonthly Cost (CAD)\nKey Considerations\n\n\n\n\n1. Single Serverless Instance (Parallel Processing)\n$35–45 ✅\nCheapest, allows parallelism but limited by single-instance performance.\n\n\n2. Separate Serverless Instances\n$110–130\nFully scalable, better for frequent workloads.\n\n\n3. Apache Airflow on Compute\n$170–200\nSelf-managed, full control, higher maintenance.\n\n\n4. Cloud Managed Orchestration\n$80–100\nBest for scalability without management overhead.\n\n\n\n\nConclusion\n\nScenario 1 with parallelism remains the cheapest (~$35–45 CAD/month).\nIt allows parallel execution when needed, reducing the risk of long-running sequential jobs.\nIt’s best if workload concurrency is manageable (e.g., not exceeding 4 vCPUs often).\nIf greater scalability is required, Scenario 4 (Managed Orchestration) would be the next best option.\n\nWould you like a cost vs. scalability chart to visualize these trade-offs? 🚀"
  },
  {
    "objectID": "posts/you-dont-need-airflow/main.html#footnotes",
    "href": "posts/you-dont-need-airflow/main.html#footnotes",
    "title": "You Don’t Need Airflow: Orchestrate Many Data Flows in R with Maestro",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nData flows through a pipeline. I’ll use the terms data flow and pipeline interchangeably.↩︎\nBy cost I’m mostly referring to cloud computing costs, although in theory it could reduce the cost of on-premise servers if it means purchasing fewer and less expensive servers.↩︎\nI’m using the term moderate enterprise loosely to mean any organization that has around 10-50 data flows that process gigabytes of data for ETL, machine learning, etc. in batch jobs occurring at hourly or daily frequencies. This is what we have at the airport and it works perfectly fine with room to grow. As jobs become heavier and/or more frequent, you may need to be run maestro multicore or break up the project into multiple maestro projects.↩︎\nAt the airport we use Apache Nifi for legacy reasons and for the occasional time that streaming is needed. Managing the scheduling of each flow is inexplicably convoluted.↩︎"
  },
  {
    "objectID": "posts/schematic/schematic-release.html",
    "href": "posts/schematic/schematic-release.html",
    "title": "Announcing… schematic",
    "section": "",
    "text": "I’m thrilled to announce the release of schematic, an R package that helps you (the developer) communicate data validation problems to non-technical users. With schematic, you can leverage tidyselect selectors and other conveniences to compare incoming data against a schema, avoiding punishing issues caused by invalid or poor quality data.\nschematic can now be installed via CRAN:\ninstall.packages(\"schematic\")\nLearn more about schematic by checking out the docs."
  },
  {
    "objectID": "posts/schematic/schematic-release.html#motivation",
    "href": "posts/schematic/schematic-release.html#motivation",
    "title": "Announcing… schematic",
    "section": "Motivation",
    "text": "Motivation\nHaving built and deployed a number of shiny apps or APIs that require users to upload data, I noticed a common pain point: how do I communicate in simple terms any issues with the data and, more importantly, what those issues are? I needed a way to present the user with error messages that satisfy two needs:\n\nSimple and non-technical: allow developers to explain the problem rather than forcing users to understand the technical aspects of each test (you don’t want to have to explain to users what is.logical means).\nHolistic checking: present all validation issues rather than stopping evaluation on the first failure.\n\nThere already exists a number of data validation packages for R, including (but not limited to) pointblank, data.validator, and validate; so why introduce a new player? schematic certainly shares similarities with many of these packages, but where I think it innovates over existing solutions is in its unique combination of the following:\n\nLightweight: Minimal dependencies with a clear focus on checking data without the bells and whistles of graphics, tables, and whatnot.\nUser-focused but developer-friendly: Developers (especially those approaching from a tidyverse mentality) will like the expressive syntax; users will appreciate the informative instructions on how to comprehensively fix data issues (no more whack-a-mole with fixing one problem only to learn there are many others).\nEasy to integrate into applications (e.g., Shiny, Plumber): Schematic returns error messages rather than reports or data.frames, meaning that you don’t need additional logic to trigger a run time error; just pass along the error message in a notification or error code."
  },
  {
    "objectID": "posts/schematic/schematic-release.html#how-it-works",
    "href": "posts/schematic/schematic-release.html#how-it-works",
    "title": "Announcing… schematic",
    "section": "How it works",
    "text": "How it works\n\n\n\n\n\n\nWarning\n\n\n\nAll R errors that appear in this post are intentional for the purpose of demonstrating schematic’s error messaging.\n\n\nSchematic is extremely simple. You only need to do two things: create a schema and then check a data.frame against the schema.\nA schema is a set of rules for columns in a data.frame. A rule consists of two parts:\n\nSelector - the column(s) on which to apply to rule\nPredicate - a function that must return a single TRUE or FALSE indicating the pass or fail of the check\n\nLet’s imagine a scenario where we have survey data and we want to ensure it matches our expectations. Here’s some sample survey data:\n\nsurvey_data &lt;- data.frame(\n  id = c(1:3, NA, 5),\n  name = c(\"Emmett\", \"Billy\", \"Sally\", \"Woolley\", \"Duchess\"),\n  age = c(19.2, 10, 22.5, 19, 19),\n  sex = c(\"M\", \"M\", \"F\", \"M\", NA),\n  q_1 = c(TRUE, FALSE, FALSE, FALSE, TRUE),\n  q_2 = c(FALSE, FALSE, TRUE, TRUE, TRUE),\n  q_3 = c(TRUE, TRUE, TRUE, TRUE, FALSE)\n)\n\nWe declare a schema using schema() and provide it with rules following the format selector ~ predicate:\n\nlibrary(schematic)\n\nmy_schema &lt;- schema(\n  id ~ is_incrementing,\n  id ~ is_all_distinct,\n  c(name, sex) ~ is.character,\n  c(id, age) ~ is_whole_number,\n  education ~ is.factor,\n  sex ~ function(x) all(x %in% c(\"M\", \"F\")),\n  starts_with(\"q_\") ~ is.logical,\n  final_score ~ is.numeric\n)\n\nThen we use check_schema to evaluate our data against the schema. Any and all errors will be captured in the error message:\n\ncheck_schema(\n  data = survey_data,\n  schema = my_schema\n)\n\nError in `check_schema()`:\n! Schema Error:\n- Columns `education` and `final_score` missing from data\n- Column `id` failed check `is_incrementing`\n- Column `age` failed check `is_whole_number`\n- Column `sex` failed check `function(x) all(x %in% c(\"M\", \"F\"))`\n\n\nThe error message will combine columns into a single statement if they share the same validation issue. schematic will also automatically report if any columns declared in the schema are missing from the data."
  },
  {
    "objectID": "posts/schematic/schematic-release.html#customizing-the-message",
    "href": "posts/schematic/schematic-release.html#customizing-the-message",
    "title": "Announcing… schematic",
    "section": "Customizing the message",
    "text": "Customizing the message\nBy default the error message is helpful for developers, but if you need to communicate the schema mismatch to a non-technical person they’ll have trouble understanding some or all of the errors. You can customize the output of each rule by inputting the rule as a named argument.\nLet’s fix up the previous example to make the messages more understandable.\n\nmy_helpful_schema &lt;- schema(\n  \"values are increasing\" = id ~ is_incrementing,\n  \"values are all distinct\" = id ~ is_all_distinct,\n  \"is a string\" = c(name, sex) ~ is.character,\n  \"is a string with specific levels\" = education ~ is.factor,\n  \"is a whole number (no decimals)\" = c(id, age) ~ is_whole_number,\n  \"has only entries 'F' or 'M'\" = sex ~ function(x) all(x %in% c(\"M\", \"F\")),\n  \"includes only TRUE or FALSE\" = starts_with(\"q_\") ~ is.logical,\n  \"is a number\" = final_score ~ is.numeric\n)\n\ncheck_schema(\n  data = survey_data,\n  schema = my_helpful_schema\n)\n\nError in `check_schema()`:\n! Schema Error:\n- Columns `education` and `final_score` missing from data\n- Column `id` failed check `values are increasing`\n- Column `age` failed check `is a whole number (no decimals)`\n- Column `sex` failed check `has only entries 'F' or 'M'`\n\n\nAnd that’s really all there is to it. schematic does come with a few handy predicate functions like is_whole_number() which is a more permissive version of is.integer() that allows for columns stored as numeric or double but still requires non-decimal values.\nMoreover, schematic includes a handful of modifiers that allow you to change the behavior of some predicates, for instance, allowing NAs with mod_nullable():\n\n# Before using `mod_nullable()` this rule triggered an error\nmy_schema &lt;- schema(\n  \"all values are increasing (except empty values)\" = id ~ mod_nullable(is_incrementing)\n)\n\ncheck_schema(\n  data = survey_data,\n  schema = my_schema\n)"
  },
  {
    "objectID": "posts/schematic/schematic-release.html#conclusion",
    "href": "posts/schematic/schematic-release.html#conclusion",
    "title": "Announcing… schematic",
    "section": "Conclusion",
    "text": "Conclusion\nIn the end, my hope is to make schematic as simple as possible and help both developers and users. It’s a package I designed initially with the sole intention of saving myself from writing validation code that takes up 80% of the actual codebase.1 I hope you find it useful too.\n\nNotes\nThis post was created using R version 4.5.0 (2025-04-11) and schematic version 0.1.0."
  },
  {
    "objectID": "posts/schematic/schematic-release.html#footnotes",
    "href": "posts/schematic/schematic-release.html#footnotes",
    "title": "Announcing… schematic",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot an exaggeration. I have a Plumber API that allows users to POST data to be processed. 80% of that plumber code is to validate the incoming data.↩︎"
  },
  {
    "objectID": "posts/maestro-0-6-0/maestro-0-6-0-release.html",
    "href": "posts/maestro-0-6-0/maestro-0-6-0-release.html",
    "title": "maestro 0.6.0",
    "section": "",
    "text": "There are a bunch of new features to share as part of the 0.6.0 release of maestro:\nIf you haven’t heard of maestro, it’s a package that helps you schedule your R scripts all in a single project using tags. You can learn more about it here.\nGet it from CRAN:\ninstall.packages(\"maestro\")"
  },
  {
    "objectID": "posts/maestro-0-6-0/maestro-0-6-0-release.html#flags",
    "href": "posts/maestro-0-6-0/maestro-0-6-0-release.html#flags",
    "title": "maestro 0.6.0",
    "section": "Flags",
    "text": "Flags\nA flag is an arbitrary string that could be used to classify or label a pipeline.1 You can now add any number of flags to a pipeline using the maestroFlags tag like so:\n\n# ./pipelines\n\n# You could use tags to classify a pipeline as critical\n#' @maestroFrequency 1 day\n#' @maestroStartTime 2024-06-03\n#' @maestroFlags critical \nsuper_important &lt;- function() {\n  # Obv. does something important\n}\n\n# You can have as many flags as you want separated by spaces\n#' @maestroFrequency hourly\n#' @maestroStartTime 2025-04-05 12:30:00\n#' @maestroFlags aviation api-access\nairlines &lt;- function() {\n  # Accesses airlines from an API or whatever\n}\n\nOnce you’ve flagged some pipelines, you can access the flags for all pipelines in the schedule as a data.frame using get_flags().\n\nlibrary(maestro)\n\nschedule &lt;- build_schedule(quiet = TRUE)\n\nget_flags(schedule)\n\n# A tibble: 3 × 2\n  pipe_name       flag      \n  &lt;chr&gt;           &lt;chr&gt;     \n1 super_important critical  \n2 airlines        aviation  \n3 airlines        api-access\n\n\nThis table could be used, for example, to send statuses reports to particular groups based on the tags, or trigger warnings/errors based on the criticality of the pipelines that failed. In these cases, it’s helpful to join the table with either get_status() or get_schedule()."
  },
  {
    "objectID": "posts/maestro-0-6-0/maestro-0-6-0-release.html#priority",
    "href": "posts/maestro-0-6-0/maestro-0-6-0-release.html#priority",
    "title": "maestro 0.6.0",
    "section": "Priority",
    "text": "Priority\nSometimes you have multiple pipelines that run at the same time - say, if you have two hourly pipelines running on the same cadence. You may want to control the order in which these pipelines are executed2. The new maestroPriority tag allows you to configure the priority in which pipelines are executed:\n\n#' @maestroFrequency 1 hour\n#' @maestroStartTime 10:00:00\nim_less_important &lt;- function() {\n  # some less important stuff\n}\n\n#' @maestroFrequency 1 hour\n#' @maestroStartTime 10:00:00\n#' @maestroPriority 1\ni_go_first &lt;- function() {\n  # this needs to happen first\n}\n\nThese pipelines run every hour on the 00 minute. The second pipeline has maestroPriority 1, indicating that it goes first when the orchestrator kicks off the pipelines. Pipelines without a priority always go last and pipelines with the same priority level use default ordering (alphabetical by script path name and then line number) within their own priority level."
  },
  {
    "objectID": "posts/maestro-0-6-0/maestro-0-6-0-release.html#slot-usage",
    "href": "posts/maestro-0-6-0/maestro-0-6-0-release.html#slot-usage",
    "title": "maestro 0.6.0",
    "section": "Slot Usage",
    "text": "Slot Usage\nAs a maestro project grows it can become increasingly difficult to know when is the best time to schedule a pipeline. You typically want to avoid scheduling a bunch of pipelines at the same time (unless they need to be executed together or at that particular time), and you don’t want a ton of empty time slots (i.e., times where the orchestrator kicks off no pipelines).\nBehold, the get_slot_usage() function!\nThis function looks ahead to all scheduled runs of pipelines in the project and returns a data.frame indicating the pipelines that are scheduled to run on each time slot. It’s easier to understand how this works in practice.\nLet’s create a bunch of pipelines first:\n\n#' ./pipelines\n#' @maestroFrequency hourly\n#' @maestroStartTime 14:00:00\nhourly &lt;- function() {\n  \n}\n\n#' @maestroFrequency daily\n#' @maestroStartTime 14:00:00\ndaily &lt;- function() {\n  \n}\n\n#' @maestroFrequency 3 hours\n#' @maestroStartTime 00:00:00\nevery_3_hours &lt;- function() {\n  \n}\n\n#' @maestroFrequency weekly\n#' @maestroStartTime 2025-05-15 04:00:00\nweekly &lt;- function() {\n  \n}\n\n#' @maestroFrequency daily\n#' @maestroDays 4 9 16 20\nsome_days &lt;- function() {\n  \n}\n\nIn this example we’re considering running the orchestrator every 1 hour and we want to see for each hour time slot what pipelines are scheduled to run:\n\nschedule &lt;- build_schedule(quiet = TRUE)\n\nget_slot_usage(\n  schedule,\n  orch_frequency = \"1 hour\",\n  slot_interval = \"hour\"\n)\n\n# A tibble: 24 × 3\n   slot  n_runs pipe_names                      \n   &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;                           \n 1 00:00      3 hourly, every_3_hours, some_days\n 2 01:00      1 hourly                          \n 3 02:00      1 hourly                          \n 4 03:00      2 hourly, every_3_hours           \n 5 04:00      2 hourly, weekly                  \n 6 05:00      1 hourly                          \n 7 06:00      2 hourly, every_3_hours           \n 8 07:00      1 hourly                          \n 9 08:00      1 hourly                          \n10 09:00      2 hourly, every_3_hours           \n# ℹ 14 more rows\n\n\nWe can see that things are fairly evenly distributed aside from the hour 00 which has 3 pipelines scheduled. There are also many times where only 1 pipeline runs, so if we have a pipeline that runs daily we’d want to schedule it at a less busy time.\nWe can change the slot_interval argument to any other valid unit of time to get a different picture.\n\nget_slot_usage(\n  schedule,\n  orch_frequency = \"1 hour\",\n  slot_interval = \"day\"\n)\n\n# A tibble: 31 × 3\n   slot  n_runs pipe_names                                     \n   &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;                                          \n 1 01         4 hourly, daily, every_3_hours, weekly           \n 2 02         4 hourly, daily, every_3_hours, weekly           \n 3 03         4 hourly, daily, every_3_hours, weekly           \n 4 04         5 hourly, daily, every_3_hours, weekly, some_days\n 5 05         4 hourly, daily, every_3_hours, weekly           \n 6 06         4 hourly, daily, every_3_hours, weekly           \n 7 07         4 hourly, daily, every_3_hours, weekly           \n 8 08         4 hourly, daily, every_3_hours, weekly           \n 9 09         5 hourly, daily, every_3_hours, weekly, some_days\n10 10         4 hourly, daily, every_3_hours, weekly           \n# ℹ 21 more rows\n\n\nA few things to consider when using get_slot_usage():\n\nIt looks at all future instances of when a pipeline will run not just the next unit of time. In the last example, a weekly pipeline appears to run every day but it’s just because all those days on any given month and year will involve running that pipeline.\nUsually you should keep orch_frequency the same as it is in your use of run_schedule(), but slot_interval could depend on what frequency a new pipeline is. In general, you should use one more frequency unit of time than your proposed pipeline. For example, if you’re planning a daily pipeline, use slot_interval = \"hour\" to identify what hour it should on.\nThis function is meant to be used interactively when you’re developing a maestro project. It doesn’t serve much value running in production."
  },
  {
    "objectID": "posts/maestro-0-6-0/maestro-0-6-0-release.html#footnotes",
    "href": "posts/maestro-0-6-0/maestro-0-6-0-release.html#footnotes",
    "title": "maestro 0.6.0",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPerhaps a more appropriate name for this concept is tag. However, it would be confusing to have a maestro tag that is itself called maestroTags.↩︎\nThis is not the same as having a DAG pipeline where pipelines are chained together.↩︎"
  },
  {
    "objectID": "posts/maestro-0-6-0/maestro-0-6-0-release.html#flexible-start-time",
    "href": "posts/maestro-0-6-0/maestro-0-6-0-release.html#flexible-start-time",
    "title": "maestro 0.6.0",
    "section": "Flexible Start Time",
    "text": "Flexible Start Time\nA minor improvement was made to the maestroStartTime tag to allow the use of HH:MM:SS formatting for timestamps. This is particularly useful if you have a pipeline that runs hourly or more frequent because the choice of start date was arbitrary. It’ll assume that the pipeline start date is the current date that the schedule was built."
  },
  {
    "objectID": "posts/maestro-0-6-0/maestro-0-6-0-release.html#conclusion",
    "href": "posts/maestro-0-6-0/maestro-0-6-0-release.html#conclusion",
    "title": "maestro 0.6.0",
    "section": "Conclusion",
    "text": "Conclusion\nCheck out the release notes for more details on what’s new in version 0.6.0. If you find any bugs or want to suggest new features and improvements, please add them here or reach out to me on LinkedIn.\nHappy orchestrating!"
  }
]