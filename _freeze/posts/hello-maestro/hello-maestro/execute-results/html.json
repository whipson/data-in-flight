{
  "hash": "b4fdb860ab3f35dba40948014410274c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hello maestro\"\nsubtitle: A new package for orchestrating data pipelines in R\nauthor: \"Will Hipson\"\ndate: \"2024-06-19\"\ndate-modified: last-modified\ncategories: [R, data engineering, data pipelines, orchestration, maestro, packages, deployment]\n---\n\n::: {.cell}\n\n:::\n\n\nI'm thrilled to announce the alpha release of [maestro](https://whipson.github.io/maestro/) - a new package for developing, scheduling, and monitoring data pipelines in R. Here, I'll walk through what maestro does and why you'd want to use it.\n\nMaestro is currently in its earliest stage of release and is not yet available on CRAN. It can be installed via Github like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremotes::install_github(\"https://github.com/whipson/maestro\")\n```\n:::\n\n\n## The Motivation\n\nLet's start with the *why*. Data engineers often have to manage multiple data pipelines - dozens or maybe even hundreds of pipelines. Scheduling and monitoring these pipelines in production poses challenges, particularly as the number of pipelines increases. Maestro takes a holistic approach by managing multiple independent pipelines in a single R project.[^1] No need to maintain several R projects or use bulky orchestration tools.\n\n[^1]: A popular R package for data pipelines that has been around for some time is [targets](https://docs.ropensci.org/targets/). Maestro is different from targets in that the focus of maestro is to orchestrate multiple *independent* data pipelines whereas targets is for a single connected pipeline. Maestro would be more suited toward ETL data engineering where targets is for complex analytics pipelines.\n\n## One Orchestrator, Many Pipelines\n\nThere are two components to a maestro project: an orchestrator and a collection of pipelines. A pipeline is a function that performs a task, such as extracting data and then storing it somewhere. The goal of maestro is to manage (i.e., schedule and monitor) multiple pipelines. This is done using an orchestrator script. The orchestrator script runs maestro commands like `build_schedule()` and `run_schedule()` to execute the pipelines and monitor them.\n\n## Getting Started\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(maestro)\n```\n:::\n\n\nIt is best to use maestro in an R project. After installing the package, you can create a new project using `create_maestro()` or in RStudio \\> New Project \\> Maestro Project. You'll find yourself in an R project with a structure like this:\n\n```         \nmy_project_directory\n├── orchestrator.R\n└── pipelines\n    └── my_pipe.R\n```\n\n### Our First Pipeline\n\nLet's take a look at `my_pipe.R` that was created:\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#' my_pipe maestro pipeline\n#'\n#' @maestroFrequency 1 day\n#' @maestroStartTime 2024-06-03\n#' @maestroTz UTC\n#' @maestroLogLevel INFO\n\nmy_pipe <- function() {\n\n  # Pipeline code\n}\n```\n:::\n\n\nThe script has been populated with R code to assign a function called my_pipe. The function has no code in the body so it won't do anything just yet. Above the function declaration are some specially formatted code comments. These are maestro tags and they're used to configure the scheduling and operation of the pipeline.[^2]\n\n[^2]: R users will recognize that maestro uses custom [roxygen2](https://roxygen2.r-lib.org/) tags.\n\nThe tags most important for scheduling are `maestroFrequency` and `maestroStartTime`. `maestroFrequency` is how often to run the pipeline. It takes a number and a time unit, like 1 day, 3 hours, 15 minutes, or 6 months. `maestroStartTime` is a Date (yyyy-mm-dd) or timestamp (yyyy-mm-dd HH:MM:SS) indicating when the schedule starts.\n\nConfiguring the start time is important if you have specific times you want it to run. If, for example, you want the pipeline to run once daily at 12:30, you'd use `@maestroStartTime 2024-06-03 12:00:00` (note here that the date part doesn't matter unless you schedule it in the future).\n\nWe won't concern ourselves with the other tags for now; just know that there are more and they all have default values.\n\nNow let's get the pipeline to do something. In the spirit of typical data engineering tasks, we'll create an ETL (Extract, Transform, Load) pipeline that gets data from a source, transforms it by adding a new column, and loads it into storage.\n\n### Making a Useful Pipeline\n\nWe'll use the open API from Environment Canada called [Geomet](https://api.weather.gc.ca/openapi?f=html) for meteorological data and we'll use DuckDB for storage. We'll need the `httr2` and `duckdb` packages for extraction and storage, respectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Located in ./pipelines/my_pipe.R\n#' @maestroFrequency 1 day\n#' @maestroStartTime 2024-06-03\ngeomet_stations <- function() {\n  \n  # Formulate the request  \n  req <- httr2::request(\n    \"https://api.weather.gc.ca/collections/climate-stations/items\"\n  ) |> \n    httr2::req_url_query(\n      limit = 1000,\n      skipGeometry = TRUE\n    )\n  \n  # Send the request and interpret the response\n  resp <- req |> \n    httr2::req_perform() |> \n    httr2::resp_body_json(simplifyVector = TRUE)\n  \n  # Get the properties element where the rectangular data is located\n  stations_dat <- resp$features$properties\n  \n  # Clean the names\n  stations_clean <- stations_dat |> \n    janitor::clean_names() |> \n    janitor::remove_empty(which = c(\"rows\", \"cols\")) |> \n    dplyr::mutate(\n      insert_time = lubridate::now(tzone = \"UTC\")\n    )\n  \n  # Connect to a local in-memory duckdb\n  conn <- DBI::dbConnect(duckdb::duckdb())\n  \n  # Create and write to a table\n  DBI::dbWriteTable(\n    conn, \n    name = \"geomet_stations_transactional\", \n    value = stations_clean\n  )\n  \n  # Test that it worked in the return\n  res <- DBI::dbGetQuery(\n    conn, \n    statement = \"\n    select * from geomet_stations_transactional\n    order by insert_time desc\n    limit 10\n    \"\n  ) |>\n    dplyr::as_tibble()\n  \n  DBI::dbDisconnect(conn)\n  \n  return(\n    res\n  )\n}\n```\n:::\n\n\n### Orchestrate It\n\nNow that we have a single useful pipeline, let's orchestrate it (in practice, we'd probably have more than one pipeline). We'll set the orchestrator to run at a daily frequency (this does not actually cause it to run daily, we need something else external to the R project to actually run it). For testing purposes, we'll then run this interactively:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Located in ./orchestrator.R\nlibrary(maestro)\n\nschedule <- build_schedule()\n\norch_result <- run_schedule(\n  schedule,\n  orch_frequency = \"1 day\"\n)\n```\n\n\n<div class=\"asciicast\" style=\"color: #B9C0CB;font-family: 'Fira Code',Monaco,Consolas,Menlo,'Bitstream Vera Sans Mono','Powerline Symbols',monospace;line-height: 1.300000\"><pre>\n<span style=\"color: #66C2CD;\">ℹ</span> 1 script successfully parsed                                                  \n                                                                                \n── Running pipelines <span style=\"color: #A8CC8C;\">▶</span>                                                          \n<span style=\"color: #A8CC8C;\">✔</span> <span style=\"color: #6F7783;\">./pipelines/my_pipe.R</span> <span style=\"color: #71BEF2;\">geomet_stations</span> <span style=\"color: #b8b8b8;\">[749ms]</span>                                 \n                                                                                \n── Pipeline execution completed <span style=\"color: #6F7783;\">■</span> | 0.761 sec elapsed                           \n<span style=\"color: #A8CC8C;\">✔</span> 1 success | <span style=\"color: #282D35;\">→</span> 0 skipped | <span style=\"color: #D290E4;\">!</span> 0 warnings | <span style=\"color: #E88388;\">✖</span> 0 errors | <span style=\"color: #66C2CD;\">◼</span> 1 total               \n────────────────────────────────────────────────────────────────────────────────\n                                                                                \n── Next scheduled pipelines <span style=\"color: #66C2CD;\">❯</span>                                                   \nPipe name | Next scheduled run                                                  \n• geomet_stations | 2024-06-21                                                  \n</pre></div>\n:::\n\n::: {.cell}\n\n:::\n\n\n### What we get back\n\nWe can see from the console output that the pipeline ran successfully. If we save it to a variable we get back a list with elements called and `status` and `artifacts`.\n\n#### \\$status\n\nThe `$status` element is a data.frame where each row is a pipeline. It has information about the status and runtime of each pipeline.\n\n\n::: {.cell}\n\n```{.r .cell-code}\norch_result$status\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 10\n  pipe_name  script_path invoked success pipeline_started    pipeline_ended     \n  <chr>      <chr>       <lgl>   <lgl>   <dttm>              <dttm>             \n1 geomet_st… ./pipeline… TRUE    TRUE    2024-06-19 14:50:51 2024-06-19 14:50:51\n# ℹ 4 more variables: errors <int>, warnings <int>, messages <int>,\n#   next_run <dttm>\n```\n\n\n:::\n:::\n\n\n#### \\$artifacts\n\n`$artifacts` is where any return values from the pipelines will be. In our case, it's the test sample of data inserted into the table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\norch_result$artifacts[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 33\n   wmo_identifier   longitude fre_stn_operator_name                 display_code\n   <chr>                <int> <chr>                                        <int>\n 1 <NA>           -1114200000 <NA>                                             8\n 2 <NA>           -1140700000 <NA>                                             4\n 3 <NA>           -1114500000 <NA>                                             9\n 4 <NA>           -1120200000 <NA>                                             4\n 5 <NA>           -1105000000 <NA>                                             6\n 6 71634           -653552000 Environnement et Changement climatiq…           NA\n 7 <NA>            -633931040 <NA>                                            NA\n 8 <NA>            -640340090 Réseau coopératif de stations climat…           NA\n 9 71603           -660517000 NAV Canada                                      NA\n10 71706           -630709000 NAV Canada                                      NA\n# ℹ 29 more variables: eng_stn_operator_acronym <chr>,\n#   fre_stn_operator_acronym <chr>, mly_first_date <chr>, hly_first_date <chr>,\n#   fre_prov_name <chr>, country <chr>, climate_identifier <chr>,\n#   publication_code <int>, timezone <chr>, last_date <chr>,\n#   hly_last_date <chr>, has_hourly_data <chr>, elevation <chr>, stn_id <int>,\n#   dly_first_date <chr>, dly_last_date <chr>, eng_stn_operator_name <chr>,\n#   station_name <chr>, eng_prov_name <chr>, mly_last_date <chr>, …\n```\n\n\n:::\n:::\n\n\n## Deployment\n\nOk, so we ran the orchestrator interactively, but this is only useful for testing. In practice, we want to deploy this on a server and have it run every day. To be clear: maestro does *not* do this for you - it just assumes that you are doing this and behaves accordingly. In other words, when you declare `orch_frequency = \"1 day\"` you are saying that you *intend* to run the orchestrator every 1 day.\n\nThe first decision to make about deployment is local vs. cloud. If you own the server it's local[^3], if you rent the hardware and connect to it remotely it's cloud. Here, we'll run through a straightforward local deployment because it requires less configuration and won't cost you anything.\n\n[^3]: Yes, this includes your personal laptop or desktop; however, it probably goes to sleep when not used. If you're using a Mac, you can use `pmset` in the command line to get around this (<https://www.dssw.co.uk/reference/pmset/>).\n\n#### Mac/Linux: cronR\n\nCron is a job scheduler for the Mac/Linux systems. You can use `cronR` to interface with it via R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cronR)\n\ncmd <- cron_rscript(\n  \"orchestrator.R\", \n  workdir = getwd()\n)\n\ncron_add(\n  cmd, \n  frequency = \"daily\",\n  id = \"maestro\",\n  ask = FALSE\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n#### Windows: taskscheduleR\n\nWindows users can use `taskscheduleR` to schedule a job via R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(taskscheduleR)\n\ntaskscheduler_create(\n  taskname = \"maestro\", \n  rscript = \"orchestrator.R\", \n  schedule = \"DAILY\",\n  exec_path = getwd()\n)\n```\n:::\n\n\n## Final Remarks\n\nIn this post, we saw how maestro can be used to orchestrate multiple pipelines in a single R project. Maestro is in its early development, but I encourage you to try it out. We're using it in production to orchestrate 18 production pipelines at the Halifax International Airport!\n\nI'd be remiss not to mention a few caveats of maestro:\n\n1.  It should only be used for light-medium scale batch jobs. If you need to do streaming and/or heavy load jobs, it's probably not for you.\n2.  Configuring the schedule for the orchestrator requires some thought. If you have several pipelines at different intervals and times you'll want to choose a frequency that makes sense. You can use `suggest_orch_frequency()` to get a heuristic suggestion based on a schedule. Even then - you need to make sure you actually run the orchestrator at this frequency. Weird things happen if your orchestrator actually runs more or less frequently than you said it would.\n\nIn later posts, I'll dig more into the deployment side of maestro, in particular cloud-based solutions. For now, please [read the docs](https://whipson.github.io/maestro/) and try out the package.\n\n#### Notes\n\nThis post was created using R version 4\\.3\\.3 \\(2024\\-02\\-29\\) and maestro version 0.0.3.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}