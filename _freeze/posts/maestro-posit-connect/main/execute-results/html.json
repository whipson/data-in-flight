{
  "hash": "fba0de61e8067bd9cb4cad4d450fc558",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Many Data Pipelines on Posit Connect with Maestro\"\nsubtitle: How to deploy and monitor many data pipelines with maestro and Posit Connect\nauthor: \"Will Hipson\"\ndate: \"2024-06-24\"\ndate-modified: last-modified\ncategories: [R, data engineering, data pipelines, orchestration, maestro, deployment, posit connect]\n---\n\n\n[Maestro](https://whipson.github.io/maestro/) is a package for scheduling, deploying, and monitoring data pipelines in a single project. Deploying maestro involves taking the project and automating it on some kind of schedule.\n\n[Posit Connect](https://posit.co/products/enterprise/connect/) is a platform for deploying and sharing data science content developed by Posit. Connect is primarily used to host content for people to interact with (i.e., reports, dashboards, etc). I think it's great for this purpose and we use it this way at the airport all the time. However, less well known is it can be used to run data pipelines and even act as a data repository.[^1] This is because you can schedule the execution of Quarto documents in Connect; and this means you can schedule the execution of any arbitrary R code to run. This blew my mind when I first realized it!\n\n[^1]: I hesitate to say 'database' because while it can store flat files (.csv and .parquet) it cannot be used to query them like in a true database.\n\nHere we'll look at a real world example of using maestro and Posit Connect together to deploy multiple data pipelines that automatically extract data from a source and store it in tables that accumulate data over time.\n\n## An Environmental Data Platform\n\nSticking with examples I've used in previous posts, we'll use the open [API from Environment Canada](https://api.weather.gc.ca/openapi?f=html) for extracting data. We're going to create 3 data pipelines that are completely independent - it's just three different data sources going into 3 different tables:\n\n1.  Daily climate observations\n2.  Hourly climate observations\n3.  Real-time hydrometric data\n\nEach of these will be used in a maestro pipeline and then transformed data will be stored as a [pin](https://pins.rstudio.com/) in Posit Connect.\n\n## Maestro Project\n\nWe'll need to install `maestro` from CRAN first.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"maestro\")\nlibrary(maestro)\n```\n:::\n\n\nYou'll want to use maestro in a brand new R project. If you're in RStudio you can go to New Project \\> Maestro Project, or you can run `create_maestro(\"environment-canada-pipelines\", type = \"Quarto\")` and then navigate to that project.\n\nWe'll have a project structure like this:\n\n```         \nenvironment-canada-pipelines\n├── environment-canada-pipelines.Rproj\n├── orchestrator.R\n└── pipelines\n    └── my_pipe.R\n```\n\n## Creating the pipelines\n\nLet's replace `my_pipe.R` with some code to extract, transform, and load data from one of the environmental APIs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' climate_hourly maestro pipeline\n#'\n#' @maestroFrequency 1 day\n#' @maestroStartTime 2024-06-25 01:00:00\n#' @maestroTz America/Halifax\n#' @maestroLogLevel INFO\nclimate_hourly <- function(board) {\n\n  last_full_day <- lubridate::today() - lubridate::days(1)\n  last_full_day_fmt <- format(last_full_day, \"%Y-%m-%dT%H:%M:%SZ\")\n\n  # Request to get climate observations for the last full hour\n  req <- httr2::request(\"https://api.weather.gc.ca/collections/climate-hourly/items\") |>\n    httr2::req_url_query(\n      CLIMATE_IDENTIFIER = 8202251, # corresponds with Halifax Int'l Airport\n      datetime = paste0(last_full_day_fmt, \"/..\"),\n      skipGeometry = TRUE,\n      LIMIT = 1000\n    )\n\n  resp <- req |>\n    httr2::req_perform() |>\n    httr2::resp_body_json(simplifyVector = TRUE)\n\n  df_raw <- resp$features$properties\n\n  df_proc <- df_raw |>\n    janitor::clean_names() |>\n    dplyr::mutate(\n      insert_time = lubridate::now(tzone = \"UTC\")\n    )\n\n  # Using a custom function for reading, appending, and writing in a single step\n  pin_append(\n    board,\n    df_proc,\n    name = \"climate_hourly_transactional\",\n    type = \"parquet\"\n  )\n}\n```\n:::\n\n\nNote the use of maestro tags to specify the scheduling. Even though the source is hourly climate, the data only updates daily, allowing us to get a previous full day's worth of hourly climate readings for a particular station.\n\nNote too that the function `climate_hourly` - our pipeline - takes an argument called `board`. This is to allow us to pass the Posit Connect board to the functions that will read and write to the table in Posit Connect. We'll see later how to use maestro to pass this to the pipeline.\n\n### Pins as database tables\n\nI won't go too in depth on pins here, suffice it to say that a pin is simply a file and we can store it in Posit Connect. We want to treat our data pins as database tables where we append new rows on top of existing ones. So the process will be read in the existing table and write out the table with the appended rows. I've encapsulated these steps in a single function called pin_append:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Reads a pin (creates if not existing) appends new rows and \n#' writes it back to the same pin \npin_append <- function(board, x, name, type = \"parquet\") {\n\n  existing <- tryCatch({\n    pins::pin_read(\n      board,\n      name\n    )\n  }, error = \\(e) {\n    warning(\"Pin '\", name, \"' does not exist. Creating.\")\n    return(NULL)\n  })\n\n  new <- dplyr::bind_rows(x, existing)\n\n  tryCatch({\n    pins::pin_write(\n      board,\n      new,\n      name = name,\n      type = type\n    )\n  }, error = \\(e) {\n    stop(\"Failed to append to pin '\", name, \"'\")\n  })\n}\n```\n:::\n\n\nWe'll use this function repeatedly throughout the project, so I'll put it in a folder called R.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}